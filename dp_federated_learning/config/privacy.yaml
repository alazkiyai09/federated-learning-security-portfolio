# Differential Privacy Configuration
# Reference: Abadi et al. "Deep Learning with Differential Privacy" (CCS 2016)

# Privacy budget parameters
privacy:
  # Delta (failure probability): typically 1/n or 1e-5
  delta: 1.0e-5

  # Target epsilon values for utility-privacy analysis
  epsilon_grid: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]

# Gradient clipping
clipping:
  # L2 norm bound C (clipping threshold)
  bound: 1.0

  # Clipping method: 'flat' or 'per_layer'
  method: 'flat'

# Noise parameters
noise:
  # Sampling probability q = batch_size / training_set_size
  sampling_rate: 0.01

  # Number of training steps (rounds * local_epochs)
  steps: 1000

  # Noise multiplier sigma (computed from epsilon, delta, q, steps)
  # Use privacy_calibration.py to compute this value
  multiplier: 1.0

# DP Strategy: 'local', 'central', or 'shuffle'
strategy: 'central'

# Privacy accounting
accounting:
  # Method: 'rdp' (Renyi DP) or 'moments' (Moments Accountant)
  method: 'rdp'

  # RDP order (alpha)
  # Higher alpha = tighter bound for small epsilon
  orders: [1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 12.0, 16.0, 20.0, 32.0, 64.0, 128.0]

# Validation against Opacus
validation:
  enabled: true
  compare_metrics: ['epsilon', 'accuracy', 'gradient_norm']
