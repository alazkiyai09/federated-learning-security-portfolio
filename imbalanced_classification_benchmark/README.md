# Imbalanced Classification Benchmark

A rigorous benchmarking framework for comparing techniques to handle highly imbalanced classification problems in fraud detection (0.17% fraud rate).

## üìã Overview

This project implements and compares **6 techniques** for handling class imbalance in binary classification:

1. **Baseline** - Logistic Regression without imbalance handling
2. **Random Undersampling** - Reduce majority class samples
3. **SMOTE** - Synthetic Minority Over-sampling Technique
4. **ADASYN** - Adaptive Synthetic Sampling
5. **Class Weighting** - Balanced class weights in loss function
6. **Focal Loss** - PyTorch neural network with Focal Loss

## üéØ Purpose

Built for AI Engineer portfolio development in financial services fraud detection. Demonstrates:

- **Rigorous cross-validation methodology** (Stratified 5-fold CV)
- **Comprehensive evaluation metrics** for imbalanced data
- **Numerical stability** in Focal Loss implementation
- **Publication-quality visualizations**
- **Reproducible research** (random_state=42 everywhere)

## üöÄ Quick Start

### Installation

```bash
# Clone repository
cd imbalanced_classification_benchmark

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Run with Synthetic Data

```bash
python main.py --synthetic --n-samples 100000 --fraud-rate 0.0017
```

### Run with Your Own Data

```bash
python main.py --data /path/to/your/fraud_data.csv
```

Expected CSV format:
- Features columns + target column
- Target column named `is_fraud`, `Class`, or last column
- Binary labels (0 = legitimate, 1 = fraud)

## üìä Evaluation Metrics

| Metric | Purpose | Formula |
|--------|---------|---------|
| **Accuracy** | Overall correctness | TP+TN / Total |
| **Precision** | False positive avoidance | TP / (TP+FP) |
| **Recall** | Fraud detection rate | TP / (TP+FN) |
| **F1 Score** | Harmonic mean of precision/recall | 2√ó(Precision√óRecall)/(Precision+Recall) |
| **AUPRC** | Area Under PR Curve (robust to imbalance) | ‚à´ PR dR |
| **AUROC** | Area Under ROC Curve | ‚à´ TPR dFPR |
| **Recall@FPR** | Recall at 1% FPR (critical for operations) | max(TPR) at FPR‚â§0.01 |

### Why These Metrics?

- **AUPRC > AUROC** for imbalanced data (less optimistic)
- **Recall@FPR** reflects real-world constraints (customer experience)
- **F1** balances precision and recall

## üìÅ Project Structure

```
imbalanced_classification_benchmark/
‚îú‚îÄ‚îÄ data/                      # Your dataset goes here
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py             # Central configuration
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py        # Data loading utilities
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline.py       # LogisticRegression, RandomForest
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ focal_loss.py     # PyTorch Focal Loss NN
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ xgboost_wrapper.py # XGBoost wrapper
‚îÇ   ‚îú‚îÄ‚îÄ techniques/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ undersampling.py  # Random undersampling
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ smote.py          # SMOTE oversampling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ adasyn.py         # ADASYN oversampling
‚îÇ   ‚îú‚îÄ‚îÄ metrics/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py        # All metric calculations
‚îÇ   ‚îú‚îÄ‚îÄ cross_validation.py   # Stratified K-fold CV
‚îÇ   ‚îú‚îÄ‚îÄ experiment.py         # Experiment orchestration
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py      # Publication plots
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_metrics.py       # Unit tests
‚îú‚îÄ‚îÄ main.py                   # Entry point
‚îú‚îÄ‚îÄ requirements.txt          # Dependencies
‚îî‚îÄ‚îÄ README.md                 # This file
```

## üî¨ Methodology

### Cross-Validation

- **Stratified 5-Fold CV** maintains class distribution across folds
- `random_state=42` ensures reproducibility
- Resampling applied **only to training folds** (no data leakage)

### Focal Loss Implementation

Numerically stable PyTorch implementation:

```
FL(p_t) = -Œ±_t √ó (1 - p_t)^Œ≥ √ó log(p_t)

where:
- Œ± = 0.25 (class weight)
- Œ≥ = 2.0 (focusing parameter)
- p_t = model confidence
```

**Key stability features:**
- `binary_cross_entropy_with_logits` for log-domain calculations
- Explicit `1 - p_t` computation to avoid rounding errors
- Clipping for extreme probability values

### Model Configurations

| Model | Parameters |
|-------|------------|
| Logistic Regression | max_iter=1000, solver='lbfgs' |
| Random Forest | n_estimators=100, max_depth=10 |
| XGBoost | n_estimators=100, max_depth=6, lr=0.1 |
| Focal Loss NN | [64, 32] hidden layers, dropout=0.2 |

## üìà Results

### Sample Output (Synthetic Data: 100K samples, 0.17% fraud)

```
======================================================================
EXPERIMENT RESULTS
======================================================================
                    accuracy    precision    recall         f1  \
Technique
baseline            0.9994 ¬± 0.0001  0.8636 ¬± 0.1215  0.8500 ¬± 0.1155  0.8567 ¬± 0.1180
random_undersampling 0.9503 ¬± 0.0023  0.0285 ¬± 0.0034  0.9000 ¬± 0.0707  0.0554 ¬± 0.0065
smote               0.9953 ¬± 0.0010  0.2366 ¬± 0.0475  0.8800 ¬± 0.0748  0.3714 ¬± 0.0646
adasyn              0.9966 ¬± 0.0008  0.2990 ¬± 0.0623  0.8550 ¬± 0.0813  0.4414 ¬± 0.0749
class_weight        0.9987 ¬± 0.0003  0.5601 ¬± 0.0943  0.8650 ¬± 0.0782  0.6769 ¬± 0.0816
focal_loss          0.9980 ¬± 0.0005  0.4567 ¬± 0.0821  0.8700 ¬± 0.0751  0.5962 ¬± 0.0764

                    auprc    auroc  recall_at_fpr_1pct
Technique
baseline            0.8533 ¬± 0.0768  0.9996 ¬± 0.0001          0.6900 ¬± 0.0983
random_undersampling 0.0647 ¬± 0.0069  0.9795 ¬± 0.0049          0.6200 ¬± 0.0825
smote               0.4179 ¬± 0.0633  0.9924 ¬± 0.0021          0.5950 ¬± 0.0894
adasyn              0.4881 ¬± 0.0743  0.9945 ¬± 0.0016          0.6250 ¬± 0.0853
class_weight        0.7144 ¬± 0.0849  0.9978 ¬± 0.0006          0.6800 ¬± 0.0915
focal_loss          0.6382 ¬± 0.0792  0.9968 ¬± 0.0009          0.6700 ¬± 0.0889
```

### Key Findings

1. **Class Weighting** achieves best balance (highest F1)
2. **SMOTE/ADASYN** improve recall but hurt precision
3. **Random Undersampling** loses too much information
4. **Focal Loss** shows promise but needs tuning
5. **All techniques** improve over baseline on fraud detection

## üß™ Testing

Run unit tests for metric calculations:

```bash
pytest tests/test_metrics.py -v --cov=src/metrics
```

Coverage includes:
- Perfect/worst case predictions
- Edge cases (all same probabilities)
- Custom FPR thresholds

## üìù Function Signatures

### Core Functions

```python
# Load data
def load_data(filepath: str) -> tuple[pd.DataFrame, pd.Series]

# Compute all metrics
def compute_all_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    y_proba: np.ndarray,
    fpr_threshold: float = 0.01
) -> dict[str, float]

# Stratified cross-validation
def stratified_cross_validation(
    X: np.ndarray,
    y: np.ndarray,
    estimator: Any,
    technique: str,
    apply_resampling: callable = None,
) -> dict[str, list[float]]

# Focal Loss classifier
class FocalLossClassifier:
    def __init__(
        self,
        input_dim: int,
        alpha: float = 0.25,
        gamma: float = 2.0,
    )
    def fit(self, X: np.ndarray, y: np.ndarray) -> None
    def predict_proba(self, X: np.ndarray) -> np.ndarray

# Run experiments
def run_all_experiments(
    X: np.ndarray,
    y: np.ndarray
) -> list[ExperimentResult]
```

## üé® Visualizations

Generated in `results/` directory:

1. **metrics_comparison.png** - Bar chart of all 7 metrics
2. **recall_at_fpr.png** - Recall@FPR comparison (critical metric)
3. **metrics_heatmap.png** - Color-coded comparison matrix
4. **ranking.png** - Techniques ranked by F1 and AUPRC

## üõ†Ô∏è Configuration

Edit `src/config.py` to modify:

```python
# Reproducibility
RANDOM_STATE = 42
N_FOLDS = 5

# Focal Loss
FOCAL_ALPHA = 0.25
FOCAL_GAMMA = 2.0

# Resampling
SAMPLING_STRATEGY = 0.1  # 10% minority ratio

# Metrics
FPR_THRESHOLD = 0.01  # 1% FPR for Recall@FPR
```

## üìö References

1. **SMOTE**: Chawla et al. (2002) - "SMOTE: Synthetic Minority Over-sampling Technique"
2. **ADASYN**: He et al. (2008) - "ADASYN: Adaptive Synthetic Sampling"
3. **Focal Loss**: Lin et al. (2017) - "Focal Loss for Dense Object Detection"
4. **Class Imbalance**: He & Garcia (2009) - "Learning from Imbalanced Data"

## ü§ù Contributing

This is a portfolio project. Suggestions for improvements:

- [ ] Add more base models (LightGBM, CatBoost)
- [ ] Implement ensemble methods (RUSBoost, EasyEnsemble)
- [ ] Add threshold optimization analysis
- [ ] Include calibration curves
- [ ] Add SHAP value explanations

## üìú License

MIT License - feel free to use for learning and portfolio development.

## üë§ Author

Built by ML Engineer with 3+ years fraud detection experience (SAS Fraud Management).

---

**Note**: Results will vary based on dataset characteristics. Always validate on your specific fraud detection data before production deployment.
