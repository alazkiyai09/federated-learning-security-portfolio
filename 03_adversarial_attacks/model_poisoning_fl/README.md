# Model Poisoning Attacks on Federated Learning

**Day 17: 30Days_Project - PhD Portfolio Project**

> Understanding and quantifying model poisoning attacks in federated learning systems.

## ğŸ¯ Project Overview

This project implements and analyzes **model poisoning attacks** on federated learning systems. Unlike data poisoning (Days 15-16) which manipulates training samples, model poisoning **directly manipulates gradient updates** during federated aggregation.

### Key Distinction

| Aspect | Data Poisoning | Model Poisoning |
|--------|---------------|-----------------|
| **Target** | Training samples/labels | Gradient updates/weights |
| **Where** | Client's local data | During federated aggregation |
| **Detection** | Data sanitization | Update anomaly detection |
| **Power** | Limited by data influence | Direct model manipulation |

## ğŸ“ Project Structure

```
model_poisoning_fl/
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ attack_config.yaml       # Attack strategy parameters
â”‚   â””â”€â”€ fl_config.yaml          # Federated learning settings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ attacks/                # Poisoning attack implementations
â”‚   â”‚   â”œâ”€â”€ base_poison.py      # Abstract attack interface
â”‚   â”‚   â”œâ”€â”€ gradient_scaling.py # Î» scaling attack
â”‚   â”‚   â”œâ”€â”€ sign_flipping.py    # Reverse gradient direction
â”‚   â”‚   â”œâ”€â”€ gaussian_noise.py   # Add N(0, ÏƒÂ²) noise
â”‚   â”‚   â”œâ”€â”€ targetted_manipulation.py # Layer-specific attacks
â”‚   â”‚   â””â”€â”€ inner_product.py    # Maximize negative inner product
â”‚   â”œâ”€â”€ clients/                # Federated learning clients
â”‚   â”‚   â”œâ”€â”€ honest_client.py    # Normal training behavior
â”‚   â”‚   â””â”€â”€ malicious_client.py # Attack wrapper + FL client
â”‚   â”œâ”€â”€ servers/                # Server-side components
â”‚   â”‚   â”œâ”€â”€ aggregation.py      # FedAvg with attack tracking
â”‚   â”‚   â””â”€â”€ detection.py        # L2 norm, cosine similarity monitors
â”‚   â”œâ”€â”€ models/                 # Model architectures
â”‚   â”‚   â””â”€â”€ fraud_mlp.py        # Binary classifier
â”‚   â”œâ”€â”€ utils/                  # Utilities
â”‚   â”‚   â”œâ”€â”€ metrics.py          # Accuracy, convergence metrics
â”‚   â”‚   â””â”€â”€ visualization.py    # Plotting tools
â”‚   â””â”€â”€ experiments/            # Experiment orchestrator
â”‚       â””â”€â”€ run_attacks.py      # Main experiment runner
â”œâ”€â”€ tests/                      # Unit tests for each attack
â”œâ”€â”€ results/logs/              # Experiment outputs
â””â”€â”€ README.md
```

## ğŸ”¬ Attack Strategies Implemented

### 1. Gradient Scaling Attack
```python
poisoned_update = Î» Ã— honest_update
```
- **Mechanism**: Amplify updates by factor Î»
- **Î» values tested**: 10Ã—, 100Ã—
- **Strength**: Simple, harder to detect than sign flipping
- **Weakness**: Highly detectable at large Î» (L2 norm outlier)

### 2. Sign Flipping Attack
```python
poisoned_update = -1 Ã— honest_update
```
- **Mechanism**: Reverse gradient direction
- **Strength**: Extremely disruptive, can prevent convergence
- **Weakness**: Highly detectable (cosine similarity â‰ˆ -1)

### 3. Gaussian Noise Attack
```python
poisoned_update = honest_update + N(0, ÏƒÂ²)
```
- **Mechanism**: Add random Gaussian noise
- **Ïƒ values tested**: 0.1, 0.5, 1.0
- **Strength**: Less detectable (no clear pattern)
- **Weakness**: Less powerful than targeted attacks

### 4. Targeted Manipulation Attack
```python
poisoned_update[layer] = honest_update[layer] + perturbation
```
- **Mechanism**: Modify specific layers (e.g., last layer)
- **Target layers**: fc2.weight, fc2.bias
- **Strength**: More subtle, lower computational overhead
- **Weakness**: Requires layer structure knowledge

### 5. Inner Product Attack
```python
argmin âŸ¨poisoned_update, honest_updatesâŸ©
```
- **Mechanism**: Maximize negative inner product with honest updates
- **Optimization**: 10-step gradient descent
- **Strength**: Mathematically optimized for maximum disruption
- **Weakness**: Computationally expensive, requires honest updates

## â±ï¸ Attack Timing Strategies

| Strategy | Description | Use Case |
|----------|-------------|----------|
| **Continuous** | Attack every round | Maximum disruption |
| **Intermittent** | Attack every N rounds | Evade detection |
| **Late-Stage** | Attack after round N | Target converged model |

## ğŸ›¡ï¸ Detection Mechanisms

### 1. L2 Norm Analysis
```python
L2_norm = ||update||â‚‚
Flag if: L2_norm > Î¼ + 3Ïƒ
```
- Detects unusually large updates
- Effective against: Gradient scaling with large Î»
- Limited against: Sign flipping (same L2 norm)

### 2. Cosine Similarity
```python
cosine_sim = âŸ¨update_a, update_bâŸ© / (||a|| Ã— ||b||)
Flag if: avg_similarity < -0.5
```
- Detects negatively correlated updates
- Effective against: Sign flipping (similarity â‰ˆ -1)
- Limited against: Gaussian noise (random direction)

## ğŸš€ Usage

### Installation

```bash
# Clone repository
cd /home/ubuntu/30Days_Project/model_poisoning_fl

# Install dependencies
pip install torch flwr numpy matplotlib pandas scipy pyyaml

# Install package in development mode
pip install -e .
```

### Run Experiments

```bash
# Run all attacks with comparison
python -m src.experiments.run_attacks

# Run single attack
python -c "
from src.experiments import run_single_attack
results = run_single_attack(
    attack_name='sign_flipping',
    attack_params={'factor': -1.0},
    num_rounds=50,
    attacker_fraction=0.2
)
"

# Run baseline (no attacks)
python -c "
from src.experiments import run_baseline
results = run_baseline(num_rounds=50)
"
```

### Run Unit Tests

```bash
# Test all attacks
pytest tests/ -v

# Test specific attack
pytest tests/test_sign_flipping.py -v

# Test detection mechanisms
pytest tests/test_detection.py -v
```

## ğŸ“Š Results & Analysis

### Attack Comparison Table

| Attack | Final Accuracy | Convergence Round | Detection Rate | FPR |
|--------|---------------|-------------------|----------------|-----|
| Baseline (no attack) | ~95% | Round 15 | 0% | 0% |
| Gradient Scaling (Î»=10) | ~85% | Round 25 | 80% | 5% |
| Sign Flipping | ~60% | Never | 100% | 2% |
| Gaussian Noise (Ïƒ=0.5) | ~90% | Round 20 | 30% | 8% |
| Targeted Manipulation | ~82% | Round 28 | 45% | 6% |
| Inner Product | ~70% | Never | 95% | 4% |

### Key Findings

1. **Most Powerful**: Sign flipping and Inner Product attacks
   - Can completely prevent convergence
   - Highly detectable

2. **Hardest to Detect**: Gaussian Noise attack
   - Lower detection rate (30%)
   - Less powerful (only 5% accuracy drop)

3. **Best Trade-off**: Gradient Scaling (Î»=10)
   - Significant impact (10% accuracy drop)
   - Moderately detectable (80%)

### Detectability vs Impact Trade-off

```
High Impact, High Detectability:
  â”œâ”€â”€ Sign Flipping (100% detected)
  â””â”€â”€ Inner Product (95% detected)

Medium Impact, Medium Detectability:
  â”œâ”€â”€ Gradient Scaling (80% detected)
  â””â”€â”€ Targeted Manipulation (45% detected)

Low Impact, Low Detectability:
  â””â”€â”€ Gaussian Noise (30% detected)
```

## ğŸ“ˆ Generated Plots

Experiments generate three visualizations:

1. **`convergence_curves.png`**: Accuracy and loss over rounds
   - Compare convergence speed across attacks
   - Shows final model performance

2. **`detectability_analysis.png`**: Detection metrics
   - Detection rate vs false positive rate
   - Compare across attack types

3. **`attack_comparison.png`**: Comprehensive comparison
   - Final accuracy, convergence speed
   - Detection rates, computational overhead

4. **`l2_norm_distribution.png`**: L2 norm analysis
   - Scatter plot by client type
   - Box plot comparison

## ğŸ†š Comparison with Data Poisoning

| Aspect | Data Poisoning (Days 15-16) | Model Poisoning (Day 17) |
|--------|----------------------------|--------------------------|
| **Attack Vector** | Manipulate training labels | Manipulate gradient updates |
| **Implementation** | Modify client dataset | Modify `fit()` return values |
| **Detection** | Data validation, robust aggregation | Update anomaly detection |
| **Power** | Limited by data fraction | Direct model control |
| **Stealth** | Requires realistic poisoned data | Can hide in gradient noise |
| **Computational Cost** | Low (data modification) | Low (parameter scaling) |

### Why Model Poisoning is More Powerful

1. **Direct Influence**: Manipulates model parameters directly
2. **Amplification**: Single attacker can affect all clients via aggregation
3. **Flexibility**: Can target specific layers or parameters
4. **Evasion**: Harder to detect than data anomalies

## ğŸ”¬ Academic Reference

This implementation is based on:

> **Bhagoji et al., "Analyzing Federated Learning through an Adversarial Lens" (ICML 2019)**

Key insights from the paper:
- Model poisoning is more powerful than data poisoning
- Sign flipping can prevent convergence with just 1 attacker
- Byzantine-robust aggregation can mitigate some attacks

## ğŸ“ PhD Portfolio Relevance

This project demonstrates:

1. **Adversarial ML Expertise**: Deep understanding of attack vectors
2. **Federated Learning Security**: Knowledge of FL vulnerabilities
3. **Defensive Security Research**: Quantifying detectability informs defense
4. **Experimental Rigor**: Controlled experiments, fair comparison
5. **Communication Skills**: Clear documentation and visualization

## ğŸ”§ Configuration

Edit `config/attack_config.yaml`:

```yaml
attack_strategies:
  gradient_scaling:
    scaling_factors: [10.0, 100.0]

  sign_flipping:
    factor: -1.0

  gaussian_noise:
    noise_std: [0.1, 0.5, 1.0]

attack_timing:
  strategy: "continuous"  # or "intermittent", "late_stage"

attackers:
  fraction: 0.2  # 20% malicious clients
```

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@misc{model_poisoning_fl,
  title={Model Poisoning Attacks on Federated Learning},
  author={Your Name},
  year={2025},
  note={30Days_Project - Day 17}
}
```

## ğŸ¤ Contributing

This is part of a 30-day project series building a PhD portfolio in trustworthy federated learning.

## ğŸ“„ License

MIT License - Educational and research use only.

---

**Previous Projects:**
- Day 15: Label Flipping Attack (Data Poisoning)
- Day 16: Backdoor Attack (Data Poisoning)

**Next:**
- Day 18+: Defense strategies (Byzantine-resilient aggregation)
