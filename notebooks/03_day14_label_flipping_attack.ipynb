{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 14: Label Flipping Attack\n",
    "\n",
    "**Data Poisoning Attack Against Federated Learning**\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Demonstrate how malicious clients can poison FL models\n",
    "- **Attack Variants**: Random flip, Targeted flip, Inverse flip\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Attack Mechanics**: How label flipping works\n",
    "2. **Attack Impact**: Effect on model performance\n",
    "3. **Stealthiness**: How to evade detection\n",
    "\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è **Ethical Disclaimer**: This notebook is for defensive research only. Understanding attacks helps build better defenses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Label Flipping Attacks\n",
    "\n",
    "### What is Label Flipping?\n",
    "\n",
    "Label flipping is a **data poisoning attack** where a malicious client changes training labels before local training.\n",
    "\n",
    "```\n",
    "Honest Client:  [features, true_label]  ‚Üí train() ‚Üí model\n",
    "Malicious:     [features, flipped_label] ‚Üí train() ‚Üí poisoned_model\n",
    "```\n",
    "\n",
    "### Why is it Effective?\n",
    "1. **Hard to Detect**: Flipped labels look like normal data\n",
    "2. **Propagates**: Poisoned gradients affect global model via aggregation\n",
    "3. **Cumulative**: Multiple malicious clients amplify the effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attack Variant 1: Random Flip\n",
    "\n",
    "Flip labels randomly with probability p (both 0‚Üí1 and 1‚Üí0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(labels: np.ndarray, flip_prob: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly flip labels with probability p.\n",
    "    \n",
    "    This creates noise in the training data but is less targeted.\n",
    "    \"\"\"\n",
    "    flipped = labels.copy()\n",
    "    \n",
    "    # Generate random mask\n",
    "    mask = np.random.random(len(labels)) < flip_prob\n",
    "    \n",
    "    # Flip selected labels (0‚Üí1, 1‚Üí0)\n",
    "    flipped[mask] = 1 - flipped[mask]\n",
    "    \n",
    "    return flipped\n",
    "\n",
    "# Example\n",
    "original_labels = np.array([0, 0, 0, 1, 1, 1, 0, 1])\n",
    "flipped_labels = random_flip(original_labels, flip_prob=0.3)\n",
    "\n",
    "print(\"Original:\", original_labels)\n",
    "print(\"Flipped (30%):\", flipped_labels)\n",
    "print(f\"Flips: {np.sum(original_labels != flipped_labels)} / len(original_labels) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attack Variant 2: Targeted Flip (Stealthy)\n",
    "\n",
    "Flip only fraud labels (1‚Üí0) to teach model to miss fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targeted_flip(labels: np.ndarray, flip_prob: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Targeted flip: Only flip fraud cases (1‚Üí0).\n",
    "    \n",
    "    This is particularly harmful for fraud detection:\n",
    "    - Teaches model: \"these fraud patterns are actually legitimate\"\n",
    "    - Reduces fraud detection rate\n",
    "    - Harder to detect than random flipping\n",
    "    \"\"\"\n",
    "    flipped = labels.copy()\n",
    "    \n",
    "    # Only flip fraud labels (where label == 1)\n",
    "    fraud_indices = labels == 1\n",
    "    \n",
    "    # Select subset of fraud to flip\n",
    "    flip_mask = np.random.random(np.sum(fraud_indices)) < flip_prob\n",
    "    \n",
    "    # Apply flip\n",
    "    fraud_locs = np.where(fraud_indices)[0]\n",
    "    for i, idx in enumerate(fraud_locs):\n",
    "        if flip_mask[i]:\n",
    "            flipped[idx] = 0\n",
    "    \n",
    "    return flipped\n",
    "\n",
    "# Example\n",
    "original_labels = np.array([0, 0, 1, 1, 1, 0, 1, 1])\n",
    "flipped_labels = targeted_flip(original_labels, flip_prob=0.5)\n",
    "\n",
    "print(\"Original:\", original_labels)\n",
    "print(\"Targeted Flipped (50% of fraud):\", flipped_labels)\n",
    "print(f\"\\nFraud cases before: {np.sum(original_labels == 1)}\")\n",
    "print(f\"Fraud cases after: {np.sum(flipped_labels == 1)}\")\n",
    "print(f\"Impact: {(1 - np.sum(flipped_labels == 1) / np.sum(original_labels == 1)) * 100:.0f}% reduction in fraud cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Variant 3: Inverse Flip (Maximum Damage)\n",
    "\n",
    "Flip ALL labels (0‚Üí1 and 1‚Üí0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_flip(labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Inverse flip: Flip ALL labels (0‚Üî1).\n",
    "    \n",
    "    Most severe attack - completely inverts the learning objective.\n",
    "    Causes maximum model degradation.\n",
    "    \"\"\"\n",
    "    return 1 - labels\n",
    "\n",
    "# Example\n",
    "original_labels = np.array([0, 0, 0, 1, 1, 1])\n",
    "flipped_labels = inverse_flip(original_labels)\n",
    "\n",
    "print(\"Original:\", original_labels)\n",
    "print(\"Inverse Flipped:\", flipped_labels)\n",
    "print(f\"\\nAll labels flipped! {np.sum(original_labels != flipped_labels)}/{len(original_labels)} changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simulating Attack Impact\n",
    "\n",
    "Let's simulate how these attacks affect a federated learning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_fl_round(\n",
    "    n_clients: int = 10,\n",
    "    n_malicious: int = 1,\n",
    "    attack_type: str = 'targeted',\n",
    "    flip_prob: float = 0.3\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Simulate one FL round with label flipping attack.\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary with attack statistics\n",
    "    \"\"\"\n",
    "    # Simulate client labels (100 samples each)\n",
    "    # 10% fraud rate, non-IID distribution\n",
    "    client_labels = []\n",
    "    for i in range(n_clients):\n",
    "        n_fraud = np.random.randint(5, 15)  # 5-15% fraud\n",
    "        labels = np.zeros(100)\n",
    "        fraud_indices = np.random.choice(100, n_fraud, replace=False)\n",
    "        labels[fraud_indices] = 1\n",
    "        client_labels.append(labels)\n",
    "    \n",
    "    # Apply attack to malicious clients\n",
    "    poisoned_labels = []\n",
    "    for i in range(n_clients):\n",
    "        if i < n_malicious:\n",
    "            if attack_type == 'random':\n",
    "                poisoned = random_flip(client_labels[i], flip_prob)\n",
    "            elif attack_type == 'targeted':\n",
    "                poisoned = targeted_flip(client_labels[i], flip_prob)\n",
    "            elif attack_type == 'inverse':\n",
    "                poisoned = inverse_flip(client_labels[i])\n",
    "        else:\n",
    "            poisoned = client_labels[i].copy()\n",
    "        poisoned_labels.append(poisoned)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_flips = sum(\n",
    "        np.sum(client_labels[i] != poisoned_labels[i]) \n",
    "        for i in range(n_clients)\n",
    "    )\n",
    "    \n",
    "    # Estimate impact on model (simplified)\n",
    "    honest_fraud_rate = np.mean([np.mean(labels) for labels in client_labels[n_malicious:]])\n",
    "    observed_fraud_rate = np.mean([np.mean(labels) for labels in poisoned_labels])\n",
    "    \n",
    "    return {\n",
    "        'n_clients': n_clients,\n",
    "        'n_malicious': n_malicious,\n",
    "        'attack_type': attack_type,\n",
    "        'flip_prob': flip_prob,\n",
    "        'total_flips': total_flips,\n",
    "        'honest_fraud_rate': honest_fraud_rate * 100,\n",
    "        'observed_fraud_rate': observed_fraud_rate * 100,\n",
    "        'impact': (honest_fraud_rate - observed_fraud_rate) * 100\n",
    "    }\n",
    "\n",
    "# Run simulations\n",
    "print(\"=\"*60)\n",
    "print(\"SIMULATING LABEL FLIPPING ATTACKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scenarios = [\n",
    "    (1, 'random', 0.2),\n",
    "    (1, 'targeted', 0.3),\n",
    "    (1, 'inverse', 1.0),\n",
    "    (3, 'targeted', 0.3),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for n_mal, attack, prob in scenarios:\n",
    "    result = simulate_fl_round(n_malicious=n_mal, attack_type=attack, flip_prob=prob)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\nüéØ Scenario: {n_mal} malicious clients, {attack} attack\")\n",
    "    print(f\"  Flip probability: {prob}\")\n",
    "    print(f\"  Labels flipped: {result['total_flips']}\")\n",
    "    print(f\"  Honest fraud rate: {result['honest_fraud_rate']:.2f}%\")\n",
    "    print(f\"  Observed fraud rate: {result['observed_fraud_rate']:.2f}%\")\n",
    "    print(f\"  ‚ö†Ô∏è Impact: {result['impact']:+.2f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Attack Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot impact comparison\n",
    "attack_names = [r[\"attack_type\"].replace(\"_\", \" \").title() for r in results]\n",
    "impacts = [r['impact'] for r in results]\n",
    "colors = ['green' if i < -5 else 'orange' if i < -10 else 'red' for i in impacts]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(impacts)), impacts, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Attack Scenario', fontsize=12)\n",
    "plt.ylabel('Impact on Fraud Rate (percentage points)', fontsize=12)\n",
    "plt.title('Label Flipping Attack Impact Comparison', fontsize=14)\n",
    "plt.xticks(range(len(attack_names)), [\n",
    "    f\"{r['n_mal']}x {r['attack_type']}\"\n",
    "    for r in results\n",
    "], rotation=45, ha='right')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.axhline(y=-5, color='orange', linestyle='--', label='5% degradation')\n",
    "plt.axhline(y=-10, color='red', linestyle='--', label='10% degradation')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Analysis:\")\n",
    "print(\"  ‚Ä¢ 1x Inverse: Most damaging (complete inversion)\")\n",
    "print(\"  ‚Ä¢ 1x Targeted (30%): Moderate damage, stealthy\")\n",
    "print(\"  ‚Ä¢ 3x Targeted (30%): Severe damage (additive effect)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Defense Strategies\n",
    "\n",
    "How to protect against label flipping attacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalous_labels(\n",
    "    client_predictions: list,\n",
    "    threshold: float = 0.5\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Detect clients with anomalous prediction distributions.\n",
    "    \n",
    "    Simple defense: If a client's fraud rate differs significantly\n",
    "    from the group average, flag them as suspicious.\n",
    "    \"\"\"\n",
    "    fraud_rates = [np.mean(preds) for preds in client_predictions]\n",
    "    mean_rate = np.mean(fraud_rates)\n",
    "    std_rate = np.std(fraud_rates)\n",
    "    \n",
    "    anomalous = []\n",
    "    for i, rate in enumerate(fraud_rates):\n",
    "        z_score = abs(rate - mean_rate) / (std_rate + 1e-6)\n",
    "        if z_score > threshold:\n",
    "            anomalous.append(i)\n",
    "    \n",
    "    return anomalous\n",
    "\n",
    "# Example detection\n",
    "np.random.seed(42)\n",
    "client_predictions = [\n",
    "    np.random.binomial(1, 0.10, 100),  # Honest (~10% fraud)\n",
    "    np.random.binomial(1, 0.10, 100),  # Honest\n",
    "    np.random.binomial(1, 0.10, 100),  # Honest\n",
    "    np.random.binomial(1, 0.05, 100),  # MALICIOUS (flipped to 5%)\n",
    "    np.random.binomial(1, 0.03, 100),  # MALICIOUS (flipped to 3%)\n",
    "]\n",
    "\n",
    "suspicious = detect_anomalous_labels(client_predictions, threshold=2.0)\n",
    "\n",
    "print(\"Detection Results:\")\n",
    "for i in range(len(client_predictions)):\n",
    "    rate = np.mean(client_predictions[i])\n",
    "    status = \"‚ö†Ô∏è SUSPICIOUS\" if i in suspicious else \"‚úÖ OK\"\n",
    "    print(f\"  Client {i}: {rate*100:.1f}% fraud rate {status}\")\n",
    "\n",
    "print(f\"\\nüéØ Detected {len(suspicious)}/{len(client_predictions)} malicious clients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Attack Variants Comparison:\n",
    "\n",
    "| Attack | Damage | Stealthiness | Detection Difficulty |\n",
    "|--------|--------|---------------|---------------------|\n",
    "| Random Flip (20%) | Medium | Low | Easy |\n",
    "| Targeted Flip (30%) | High | High | Medium |\n",
    "| Inverse Flip | Very High | Very Low | Easy |\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Federated Learning is Vulnerable**: No central data validation\n",
    "2. **Targeted Attacks are Stealthy**: Only flip fraud‚Üílegitimate\n",
    "3. **Impact Accumulates**: Multiple malicious clients amplify damage\n",
    "4. **Defense is Possible**: Anomaly detection + robust aggregation\n",
    "\n",
    "### Real-World Implications:\n",
    "\n",
    "- **Banks**: Compromised client could hide fraud transactions\n",
    "- **Healthcare**: Poisoned model could misclassify patients\n",
    "- **Autonomous Vehicles**: Safety-critical systems at risk\n",
    "\n",
    "### Learn Defenses:\n",
    "\n",
    "‚Üí **Day 17**: Byzantine-Robust Aggregation (Krum, Trimmed Mean)\n",
    "‚Üí **Day 18**: Anomaly Detection Systems\n",
    "‚Üí **Day 19**: FoolsGold (Sybil-resistant aggregation)\n",
    "\n",
    "---\n",
    "\n",
    "**üìÅ Project Location**: `03_adversarial_attacks/label_flipping_attack/`\n",
    "\n",
    "**üìö Research Paper**: \"Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Statistical Learning\" (Jagupski et al., 2018)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
