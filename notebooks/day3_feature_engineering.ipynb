{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Feature Engineering for Fraud Detection\n",
    "\n",
    "**Advanced Feature Engineering Techniques for Transaction Fraud Detection**\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Learn advanced feature engineering for fraud detection\n",
    "- **Techniques**: Velocity features, Deviation features, Merchant encoding\n",
    "- **Goal**: Improve model performance with engineered features\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Velocity Features**: Transaction frequency and amounts over time windows\n",
    "2. **Deviation Features**: Compare current behavior to historical patterns\n",
    "3. **Merchant Features**: Target encoding and risk profiling\n",
    "4. **Feature Selection**: SHAP-based importance ranking\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic transaction data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create 10000 transactions across 100 users\n",
    "n_transactions = 10000\n",
    "n_users = 100\n",
    "\n",
    "# Generate timestamps over 30 days\n",
    "base_date = pd.Timestamp('2024-01-01')\n",
    "timestamps = [base_date + pd.Timedelta(days=np.random.randint(0, 30), \n",
    "                                       hours=np.random.randint(0, 24), \n",
    "                                       minutes=np.random.randint(0, 60)) \n",
    "              for _ in range(n_transactions)]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'user_id': np.random.randint(0, n_users, n_transactions),\n",
    "    'timestamp': sorted(timestamps),\n",
    "    'amount': np.random.lognormal(3, 1.5, n_transactions).clip(0, 10000),\n",
    "    'merchant_id': np.random.randint(0, 50, n_transactions),\n",
    "    'hour_of_day': [t.hour for t in timestamps],\n",
    "    'is_fraud': np.random.choice([0, 1], n_transactions, p=[0.95, 0.05])\n",
    "})\n",
    "\n",
    "# Add fraud patterns (higher amounts and unusual hours)\n",
    "fraud_indices = df[df['is_fraud'] == 1].index\n",
    "df.loc[fraud_indices, 'amount'] *= np.random.uniform(1.5, 3, len(fraud_indices))\n",
    "df.loc[fraud_indices, 'hour_of_day'] = np.random.choice([2, 3, 4, 22, 23], len(fraud_indices))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nFraud rate: {df['is_fraud'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Velocity Features\n",
    "\n",
    "Track transaction patterns over time windows (count, sum, mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_velocity_features(df, user_col='user_id', datetime_col='timestamp', \n",
    "                              amount_col='amount', time_windows=[(1, 'h'), (24, 'h')]):\n",
    "    \"\"\"\n",
    "    Compute velocity features: transaction frequency and amounts over time windows.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "    df = df.sort_values([user_col, datetime_col])\n",
    "    \n",
    "    result = df.copy()\n",
    "    \n",
    "    for window_size, unit in time_windows:\n",
    "        window_str = f\"{window_size}{unit}\"\n",
    "        \n",
    "        # Group by user and compute rolling statistics\n",
    "        grouped = df.groupby(user_col, group_keys=False)\n",
    "        \n",
    "        # Transaction count in window\n",
    "        rolling_count = grouped[amount_col].rolling(\n",
    "            window=window_str, min_periods=1, on=datetime_col\n",
    "        ).count()\n",
    "        result[f\"velocity_count_{window_str}\"] = rolling_count.values\n",
    "        \n",
    "        # Total amount in window\n",
    "        rolling_sum = grouped[amount_col].rolling(\n",
    "            window=window_str, min_periods=1, on=datetime_col\n",
    "        ).sum()\n",
    "        result[f\"velocity_sum_{window_str}\"] = rolling_sum.values\n",
    "        \n",
    "        # Average amount in window\n",
    "        rolling_mean = grouped[amount_col].rolling(\n",
    "            window=window_str, min_periods=1, on=datetime_col\n",
    "        ).mean()\n",
    "        result[f\"velocity_mean_{window_str}\"] = rolling_mean.values\n",
    "    \n",
    "    # Time since last transaction\n",
    "    df['prev_timestamp'] = df.groupby(user_col)[datetime_col].shift(1)\n",
    "    result['velocity_time_since_last_s'] = (\n",
    "        df[datetime_col] - df['prev_timestamp']\n",
    "    ).dt.total_seconds()\n",
    "    result['velocity_time_since_last_s'] = result['velocity_time_since_last_s'].fillna(999999)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply velocity features\n",
    "df_with_velocity = compute_velocity_features(df)\n",
    "\n",
    "print(\"Velocity Features Added:\")\n",
    "velocity_cols = [c for c in df_with_velocity.columns if c.startswith('velocity_')]\n",
    "print(velocity_cols)\n",
    "\n",
    "print(\"\\nSample velocity features:\")\n",
    "print(df_with_velocity[['user_id', 'timestamp', 'amount'] + velocity_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deviation Features\n",
    "\n",
    "Compare current transaction to user's historical behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deviation_features(df, user_col='user_id', features=['amount', 'hour_of_day'], window_size=30):\n",
    "    \"\"\"\n",
    "    Compute deviation features: compare current to historical patterns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for feature in features:\n",
    "        # Compute per-user statistics\n",
    "        user_stats = df.groupby(user_col)[feature].agg(\n",
    "            mean='mean',\n",
    "            std='std',\n",
    "            q25=lambda x: x.quantile(0.25),\n",
    "            q50=lambda x: x.quantile(0.50),\n",
    "            q75=lambda x: x.quantile(0.75)\n",
    "        )\n",
    "        \n",
    "        # Handle zero std\n",
    "        user_stats['std'] = user_stats['std'].fillna(1.0)\n",
    "        user_stats['std'] = user_stats['std'].replace(0, 1.0)\n",
    "        \n",
    "        # Merge stats\n",
    "        df_merged = df.merge(user_stats, left_on=user_col, right_index=True, suffixes=('', '_hist'))\n",
    "        \n",
    "        # Compute z-score\n",
    "        result[f\"deviation_{feature}_zscore\"] = (\n",
    "            (df_merged[feature] - df_merged[f'{feature}_hist_mean']) / df_merged[f'{feature}_hist_std']\n",
    "        )\n",
    "        \n",
    "        # Compute ratio\n",
    "        result[f\"deviation_{feature}_ratio\"] = (\n",
    "            df_merged[feature] / df_merged[f'{feature}_hist_mean'].replace(0, 1)\n",
    "        )\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply deviation features\n",
    "deviation_features = compute_deviation_features(df)\n",
    "\n",
    "print(\"Deviation Features Added:\")\n",
    "deviation_cols = list(deviation_features.columns)\n",
    "print(deviation_cols)\n",
    "\n",
    "print(\"\\nSample deviation features:\")\n",
    "print(pd.concat([df[['user_id', 'amount', 'hour_of_day']], deviation_features], axis=1).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merchant Risk Features\n",
    "\n",
    "Target encoding based on fraud rate per merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_merchant_risk_features(df, merchant_col='merchant_id', label_col='is_fraud', min_count=5):\n",
    "    \"\"\"\n",
    "    Compute merchant risk features using target encoding.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Compute fraud rate per merchant\n",
    "    merchant_stats = df.groupby(merchant_col)[label_col].agg(\n",
    "        fraud_rate='mean',\n",
    "        count='count'\n",
    "    )\n",
    "    \n",
    "    # Smooth encoding (blend with global rate)\n",
    "    global_fraud_rate = df[label_col].mean()\n",
    "    smoothing = 10\n",
    "    \n",
    "    merchant_stats['fraud_rate_smoothed'] = (\n",
    "        (merchant_stats['fraud_rate'] * merchant_stats['count'] + global_fraud_rate * smoothing) /\n",
    "        (merchant_stats['count'] + smoothing)\n",
    "    )\n",
    "    \n",
    "    # Merge back\n",
    "    df['merchant_fraud_rate'] = df.merge(\n",
    "        merchant_stats[['fraud_rate_smoothed']],\n",
    "        left_on=merchant_col,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )['fraud_rate_smoothed'].fillna(global_fraud_rate)\n",
    "    \n",
    "    # Risk bins\n",
    "    df['merchant_risk_level'] = pd.cut(\n",
    "        df['merchant_fraud_rate'],\n",
    "        bins=[0, 0.01, 0.05, 1.0],\n",
    "        labels=['Low', 'Medium', 'High']\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply merchant risk features\n",
    "df_with_merchant = compute_merchant_risk_features(df_with_velocity)\n",
    "\n",
    "print(\"Merchant Risk Features Added:\")\n",
    "print(df_with_merchant[['merchant_id', 'merchant_fraud_rate', 'merchant_risk_level']].head(10))\n",
    "\n",
    "print(\"\\nRisk Level Distribution:\")\n",
    "print(df_with_merchant['merchant_risk_level'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "df_final = pd.concat([\n",
    "    df_with_velocity.reset_index(drop=True),\n",
    "    deviation_features.reset_index(drop=True)\n",
    "], axis=1)\n",
    "\n",
    "# Select feature columns\n",
    "feature_cols = [\n",
    "    'amount', 'hour_of_day',\n",
    "    'velocity_count_1h', 'velocity_sum_1h', 'velocity_mean_1h',\n",
    "    'velocity_count_24h', 'velocity_sum_24h', 'velocity_mean_24h',\n",
    "    'velocity_time_since_last_s',\n",
    "    'deviation_amount_zscore', 'deviation_amount_ratio',\n",
    "    'deviation_hour_of_day_zscore',\n",
    "    'merchant_fraud_rate'\n",
    "]\n",
    "\n",
    "# Drop rows with NaN\n",
    "df_final = df_final.dropna(subset=feature_cols)\n",
    "\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"\\nFeature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)\n",
    "\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(df_final[feature_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model with Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = df_final[feature_cols].values\n",
    "y = df_final['is_fraud'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL PERFORMANCE WITH ENGINEERED FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy: {np.mean(y_pred == y_test)*100:.1f}%\")\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_prob):.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance = model.feature_importances_\n",
    "indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('Feature Importance (Random Forest)', fontsize=14)\n",
    "plt.bar(range(len(feature_cols)), importance[indices], color='steelblue', alpha=0.7)\n",
    "plt.xticks(range(len(feature_cols)), [feature_cols[i] for i in indices], rotation=45, ha='right')\n",
    "plt.xlabel('Feature', fontsize=12)\n",
    "plt.ylabel('Importance', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for i in indices[:5]:\n",
    "    print(f\"  {feature_cols[i]:.<40} {importance[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Feature Engineering Techniques Covered:\n",
    "\n",
    "1. **Velocity Features**: Track transaction patterns over time\n",
    "   - Count, sum, mean over rolling windows\n",
    "   - Time since last transaction\n",
    "\n",
    "2. **Deviation Features**: Detect anomalous behavior\n",
    "   - Z-score: (current - mean) / std\n",
    "   - Ratio: current / historical_mean\n",
    "\n",
    "3. **Merchant Risk Features**: Target encoding\n",
    "   - Smoothed fraud rate per merchant\n",
    "   - Risk level bins\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- âœ… **Velocity features** capture burst patterns (high frequency = suspicious)\n",
    "- âœ… **Deviation features** detect anomalies (large z-score = suspicious)\n",
    "- âœ… **Merchant features** encode domain knowledge (risky merchants)\n",
    "- âœ… **Feature importance** reveals what drives predictions\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Scale features**: Z-scores and ratios need different scales\n",
    "2. **Handle missing**: Impute or drop NaN in rolling features\n",
    "3. **Smooth encoding**: Blend target encoding with global rate\n",
    "4. **Validate**: Check feature importance to avoid leakage\n",
    "\n",
    "### Next Steps:\n",
    "â†’ **Day 4**: Classification Benchmark (compare models)\n",
    "â†’ **Day 5**: LSTM Autoencoder (sequence modeling)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“ Project Location**: `01_fraud_detection_core/fraud_feature_engineering/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
