{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Anomaly Detection for Fraud\n",
    "\n",
    "**Unsupervised Fraud Detection Using Isolation Forest**\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Detect fraud without labeled data\n",
    "- **Method**: Isolation Forest (unsupervised anomaly detection)\n",
    "- **Advantage**: Works with NO fraud labels needed during training\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Isolation Forest**: How it isolates anomalies\n",
    "2. **Training**: Train on legitimate data only\n",
    "3. **Detection**: Score new transactions for anomaly\n",
    "4. **Ensemble Methods**: Combine multiple detectors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Isolation Forest\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "1. **Random Partitioning**: Recursively split data with random thresholds\n",
    "2. **Path Length**: Anomalies are isolated quickly (short paths)\n",
    "3. **Scoring**: Shorter path = more anomalous\n",
    "\n",
    "**Key Insight**: Anomalies are \"few and different\", so they're easier to isolate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how Isolation Forest works\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 2D data\n",
    "normal = np.random.randn(200, 2) * 1.5\n",
    "anomalies = np.array([[3, 3], [-3, 3], [3, -3], [-4, -2], [5, 0]])\n",
    "\n",
    "X = np.vstack([normal, anomalies])\n",
    "y = np.array([0]*200 + [1]*5)\n",
    "\n",
    "# Fit Isolation Forest\n",
    "clf = IsolationForest(contamination=0.02, random_state=42)\n",
    "clf.fit(X)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X)\n",
    "anomaly_scores = -clf.score_samples(X)  # Convert to positive scores\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(normal[:, 0], normal[:, 1], c='blue', alpha=0.5, label='Normal')\n",
    "plt.scatter(anomalies[:, 0], anomalies[:, 1], c='red', s=100, label='Anomalies', edgecolors='black')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('True Labels', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=anomaly_scores, cmap='YlOrRd', alpha=0.7)\n",
    "plt.colorbar(scatter, label='Anomaly Score')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Isolation Forest Anomaly Scores', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Isolation Forest Results:\")\n",
    "print(f\"  Detected {np.sum(y_pred == -1)} anomalies\")\n",
    "print(f\"  Actual anomalies: {np.sum(y == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Fraud Detection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transaction data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Normal transactions\n",
    "n_normal = 5000\n",
    "normal_transactions = {\n",
    "    'amount': np.random.lognormal(3, 0.8, n_normal).clip(0, 5000),\n",
    "    'hour': np.random.randint(0, 24, n_normal),\n",
    "    'day_of_week': np.random.randint(0, 7, n_normal),\n",
    "    'merchant_category': np.random.randint(0, 10, n_normal),\n",
    "}\n",
    "\n",
    "# Fraud transactions (anomalous behavior)\n",
    "n_fraud = 100\n",
    "fraud_transactions = {\n",
    "    'amount': np.random.lognormal(5, 0.5, n_fraud).clip(1000, 15000),  # Higher amounts\n",
    "    'hour': np.random.choice([0, 1, 2, 3, 4, 22, 23], n_fraud),  # Unusual hours\n",
    "    'day_of_week': np.random.randint(0, 7, n_fraud),\n",
    "    'merchant_category': np.random.randint(0, 10, n_fraud),\n",
    "}\n",
    "\n",
    "# Combine\n",
    "df = pd.DataFrame({**{k: np.concatenate([v, fraud_transactions[k]]) for k, v in normal_transactions.items()}})\n",
    "df['is_fraud'] = [0]*n_normal + [1]*n_fraud\n",
    "\n",
    "# Shuffle\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['is_fraud'].mean()*100:.2f}%\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Isolation Forest (Legitimate Data Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Train only on legitimate data (Class=0)\n",
    "legitimate_data = df[df['is_fraud'] == 0].drop('is_fraud', axis=1)\n",
    "\n",
    "# Fit Isolation Forest\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    max_samples='auto',\n",
    "    contamination=0.02,  # Expected anomaly rate\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "iso_forest.fit(legitimate_data)\n",
    "\n",
    "print(\"âœ… Isolation Forest trained on legitimate data only!\")\n",
    "print(f\"   Trained on {len(legitimate_data)} legitimate transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detect Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anomaly scores for all data\n",
    "X_all = df.drop('is_fraud', axis=1)\n",
    "\n",
    "# Isolation Forest returns negative scores (lower = more anomalous)\n",
    "# Convert to positive scores (higher = more anomalous)\n",
    "anomaly_scores = -iso_forest.score_samples(X_all)\n",
    "\n",
    "# Binary predictions\n",
    "predictions = iso_forest.predict(X_all)\n",
    "predictions_binary = (predictions == -1).astype(int)  # -1 = anomaly\n",
    "\n",
    "print(\"Anomaly Detection Results:\")\n",
    "print(f\"  Total transactions: {len(df)}\")\n",
    "print(f\"  Detected as anomalous: {np.sum(predictions_binary)}\")\n",
    "print(f\"  Actual fraud cases: {df['is_fraud'].sum()}\")\n",
    "print(f\"  Fraud detection rate: {np.sum(predictions_binary[df['is_fraud'] == 1]) / df['is_fraud'].sum() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "y_true = df['is_fraud'].values\n",
    "y_pred_binary = predictions_binary\n",
    "\n",
    "auc = roc_auc_score(y_true, anomaly_scores)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ISOLATION FOREST - EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAUC-ROC: {auc:.3f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_binary, \n",
    "                          target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nFraud Recall: {cm[1,1] / (cm[1,0] + cm[1,1]) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimize Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_recall_curve\n",
    "\n",
    "# Find optimal threshold\n",
    "thresholds = np.percentile(anomaly_scores, np.arange(95, 100, 0.5))\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    preds = (anomaly_scores >= threshold).astype(int)\n",
    "    f1 = f1_score(y_true, preds)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores, 'o-', linewidth=2, markersize=4)\n",
    "plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best threshold = {best_threshold:.2f}')\n",
    "plt.xlabel('Anomaly Score Threshold', fontsize=12)\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.title('F1 Score vs Threshold', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Apply optimal threshold\n",
    "optimal_preds = (anomaly_scores >= best_threshold).astype(int)\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold:.3f}\")\n",
    "print(f\"Optimal F1-Score: {best_f1:.3f}\")\n",
    "print(f\"Fraud recall at optimal: {confusion_matrix(y_true, optimal_preds)[1,1] / (confusion_matrix(y_true, optimal_preds)[1,0] + confusion_matrix(y_true, optimal_preds)[1,1]) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble of Anomaly Detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data for SVM\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# Train multiple detectors\n",
    "print(\"Training ensemble of anomaly detectors...\")\n",
    "\n",
    "# 1. Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.02, random_state=42)\n",
    "iso_forest.fit(legitimate_data)\n",
    "iso_scores = -iso_forest.score_samples(X_all)\n",
    "\n",
    "# 2. One-Class SVM (trained on legitimate data)\n",
    "oc_svm = OneClassSVM(nu=0.02, kernel='rbf')\n",
    "oc_svm.fit(scaler.transform(legitimate_data))\n",
    "svm_scores = -oc_svm.score_samples(X_scaled)\n",
    "\n",
    "# 3. Local Outlier Factor\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.02, novelty=True)\n",
    "lof.fit(legitimate_data)\n",
    "lof_scores = -lof.score_samples(X_all)\n",
    "\n",
    "# Normalize scores\n",
    "iso_scores_norm = (iso_scores - iso_scores.min()) / (iso_scores.max() - iso_scores.min())\n",
    "svm_scores_norm = (svm_scores - svm_scores.min()) / (svm_scores.max() - svm_scores.min())\n",
    "lof_scores_norm = (lof_scores - lof_scores.min()) / (lof_scores.max() - lof_scores.min())\n",
    "\n",
    "# Ensemble (average)\n",
    "ensemble_scores = (iso_scores_norm + svm_scores_norm + lof_scores_norm) / 3\n",
    "\n",
    "# Compare\n",
    "methods = {\n",
    "    'Isolation Forest': iso_scores,\n",
    "    'One-Class SVM': svm_scores,\n",
    "    'Local Outlier Factor': lof_scores,\n",
    "    'Ensemble': ensemble_scores\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANOMALY DETECTION METHODS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, scores in methods.items():\n",
    "    auc = roc_auc_score(y_true, scores)\n",
    "    print(f\"{name:.<30} AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, scores) in enumerate(methods.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot histograms for each class\n",
    "    ax.hist(scores[y_true == 0], bins=50, alpha=0.6, label='Legitimate', color='blue')\n",
    "    ax.hist(scores[y_true == 1], bins=20, alpha=0.8, label='Fraud', color='red')\n",
    "    ax.set_xlabel('Anomaly Score', fontsize=11)\n",
    "    ax.set_ylabel('Count', fontsize=11)\n",
    "    ax.set_title(f'{name} (AUC={roc_auc_score(y_true, scores):.3f})', fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  â€¢ Isolation Forest: Good separation between classes\")\n",
    "print(\"  â€¢ Ensemble: Best performance by combining multiple methods\")\n",
    "print(\"  â€¢ Overlap: Some fraud transactions have low anomaly scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Anomaly Detection Methods:\n",
    "\n",
    "1. **Isolation Forest**: \n",
    "   - Random partitioning, anomalies isolated quickly\n",
    "   - Fast, scalable, works well with high-dimensional data\n",
    "\n",
    "2. **One-Class SVM**:\n",
    "   - Learns decision boundary around normal data\n",
    "   - Slower, sensitive to kernel choice\n",
    "\n",
    "3. **Local Outlier Factor**:\n",
    "   - Density-based, compares to neighbors\n",
    "   - Good for local anomalies\n",
    "\n",
    "4. **Ensemble**:\n",
    "   - Combines multiple methods\n",
    "   - More robust, better performance\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- âœ… **Unsupervised**: No fraud labels needed for training\n",
    "- âœ… **Train on legitimate**: Model learns \"normal\" behavior\n",
    "- âœ… **High anomaly score = suspicious**: Score indicates fraud likelihood\n",
    "- âœ… **Ensemble works best**: Combine multiple detectors\n",
    "\n",
    "### When to Use Anomaly Detection:\n",
    "\n",
    "1. **Limited labeled fraud**: Few fraud examples available\n",
    "2. **New fraud types**: Unknown fraud patterns\n",
    "3. **Baseline detector**: Before deploying supervised models\n",
    "4. **Ensemble component**: Combine with supervised methods\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- âŒ Lower precision than supervised methods\n",
    "- âŒ Higher false positive rate\n",
    "- âŒ Requires tuning contamination parameter\n",
    "\n",
    "### Next Steps:\n",
    "â†’ **Day 7**: Model Explainability (SHAP, LIME)\n",
    "â†’ **Day 17**: Byzantine-Robust Aggregation (FL defenses)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“ Project Location**: `01_fraud_detection_core/anomaly_detection_benchmark/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
