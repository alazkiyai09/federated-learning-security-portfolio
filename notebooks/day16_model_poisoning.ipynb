{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 16: Model Poisoning Attacks\n",
    "\n",
    "**Direct Gradient Manipulation in Federated Learning**\n",
    "\n",
    "## Overview\n",
    "- **Attack**: Manipulate gradient updates directly (not data)\n",
    "- **Target**: Global model weights during FL aggregation\n",
    "- **Power**: More powerful than data poisoning\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Gradient Scaling**: Amplify updates\n",
    "2. **Sign Flipping**: Reverse gradient direction\n",
    "3. **Inner Product Attack**: Optimized for maximum damage\n",
    "4. **Detection**: How to identify poisoned updates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Poisoning vs Data Poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë               DATA POISONING vs MODEL POISONING                       ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "DATA POISONING (Days 14-15):\n",
    "  ‚Ä¢ Target: Training samples/labels\n",
    "  ‚Ä¢ Location: Client's local data\n",
    "  ‚Ä¢ Mechanism: Change labels, add backdoor triggers\n",
    "  ‚Ä¢ Detection: Data validation, anomaly detection\n",
    "  ‚Ä¢ Power: Limited by data influence\n",
    "\n",
    "MODEL POISONING (Today):\n",
    "  ‚Ä¢ Target: Gradient updates/weights\n",
    "  ‚Ä¢ Location: During federated aggregation\n",
    "  ‚Ä¢ Mechanism: Directly manipulate sent updates\n",
    "  ‚Ä¢ Detection: Update anomaly detection (L2 norm, cosine similarity)\n",
    "  ‚Ä¢ Power: Direct control over model parameters\n",
    "\n",
    "KEY DIFFERENCE:\n",
    "  Data poisoning poisons the SOURCE (data)\n",
    "  Model poisoning poisons the PROCESS (updates)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attack Vector Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate honest vs malicious updates\n",
    "np.random.seed(42)\n",
    "\n",
    "# Honest update (small, in correct direction)\n",
    "honest_update = np.random.randn(10) * 0.1\n",
    "\n",
    "# Attack 1: Gradient scaling (amplify by 100x)\n",
    "scaled_update = honest_update * 100\n",
    "\n",
    "# Attack 2: Sign flipping (reverse direction)\n",
    "flipped_update = -honest_update\n",
    "\n",
    "# Attack 3: Gaussian noise\n",
    "noisy_update = honest_update + np.random.randn(10) * 2\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "updates = [honest_update, scaled_update, flipped_update, noisy_update]\n",
    "titles = ['Honest Update', 'Gradient Scaling (100x)', 'Sign Flipping', 'Gaussian Noise']\n",
    "\n",
    "for idx, (ax, update, title) in enumerate(zip(axes, updates, titles)):\n",
    "    ax.bar(range(10), update, color=['green' if idx == 0 else 'red'])\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel('Parameter Index', fontsize=10)\n",
    "    ax.set_ylabel('Update Value', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add L2 norm annotation\n",
    "    l2_norm = np.linalg.norm(update)\n",
    "    ax.text(0.5, 0.95, f'L2 Norm: {l2_norm:.3f}', \n",
    "            transform=ax.transAxes, ha='center', va='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat' if idx == 0 else 'lightcoral', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  ‚Ä¢ Honest: Small L2 norm, random direction\")\n",
    "print(\"  ‚Ä¢ Scaling: Same direction, 100x larger (EASY TO DETECT)\")\n",
    "print(\"  ‚Ä¢ Flipping: Opposite direction (VERY DAMAGING)\")\n",
    "print(\"  ‚Ä¢ Noise: Different direction, similar magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inner Product Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "\n",
    "INNER PRODUCT ATTACK (Most Sophisticated):\n",
    "\n",
    "Objective: Find update that MINIMIZES inner product with honest updates\n",
    "\n",
    "  maximize:  -‚ü®poisoned_update, Œ£ honest_updates‚ü©\n",
    "\n",
    "Intuition:\n",
    "  ‚Ä¢ FedAvg aggregates: w_new = w_old + Œ£(weight_i * update_i)\n",
    "  ‚Ä¢ If poisoned_update is opposite to honest_updates:\n",
    "    - They cancel each other out\n",
    "    - Global model doesn't learn\n",
    "    - Convergence prevented\n",
    "\n",
    "Optimization:\n",
    "  poisoned_update* = argmin_u Œ£‚ü®u, honest_i‚ü©\n",
    "                     \n",
    "  Solution: poisoned_update = -Œ£(honest_updates)\n",
    "            \n",
    "            (opposite to sum of honest updates)\n",
    "\n",
    "Power:\n",
    "  ‚Ä¢ Mathematically optimal for preventing convergence\n",
    "  ‚Ä¢ Harder to detect than sign flipping (similar magnitude)\n",
    "  ‚Ä¢ Can be scaled to overcome aggregation\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attack Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalous_update(client_update, honest_updates, threshold=3.0):\n",
    "    \"\"\"\n",
    "    Detect anomalous client updates.\n",
    "    \n",
    "    Methods:\n",
    "    1. L2 norm outlier detection\n",
    "    2. Cosine similarity (direction anomaly)\n",
    "    3. Euclidean distance from mean\n",
    "    \"\"\"\n",
    "    \n",
    "    # Method 1: L2 norm\n",
    "    l2_norm = np.linalg.norm(client_update)\n",
    "    l2_norms = [np.linalg.norm(u) for u in honest_updates]\n",
    "    l2_mean = np.mean(l2_norms)\n",
    "    l2_std = np.std(l2_norms)\n",
    "    l2_z_score = abs(l2_norm - l2_mean) / (l2_std + 1e-10)\n",
    "    \n",
    "    # Method 2: Cosine similarity\n",
    "    mean_direction = np.mean(honest_updates, axis=0)\n",
    "    mean_direction /= (np.linalg.norm(mean_direction) + 1e-10)\n",
    "    client_direction = client_update / (np.linalg.norm(client_update) + 1e-10)\n",
    "    cosine_sim = np.dot(mean_direction, client_direction)\n",
    "    \n",
    "    # Method 3: Euclidean distance\n",
    "    mean_update = np.mean(honest_updates, axis=0)\n",
    "    euclidean_dist = np.linalg.norm(client_update - mean_update)\n",
    "    distances = [np.linalg.norm(u - mean_update) for u in honest_updates]\n",
    "    dist_mean = np.mean(distances)\n",
    "    dist_std = np.std(distances)\n",
    "    dist_z_score = (euclidean_dist - dist_mean) / (dist_std + 1e-10)\n",
    "    \n",
    "    return {\n",
    "        'l2_norm': l2_norm,\n",
    "        'l2_z_score': l2_z_score,\n",
    "        'l2_anomalous': l2_z_score > threshold,\n",
    "        'cosine_similarity': cosine_sim,\n",
    "        'cosine_anomalous': cosine_sim < -0.5,  # Opposite direction\n",
    "        'euclidean_distance': euclidean_dist,\n",
    "        'euclidean_z_score': dist_z_score,\n",
    "        'euclidean_anomalous': dist_z_score > threshold\n",
    "    }\n",
    "\n",
    "# Test detection\n",
    "honest_updates = [np.random.randn(100) * 0.1 for _ in range(5)]\n",
    "malicious_update = honest_updates[0] * 100  # Scaled attack\n",
    "\n",
    "detection = detect_anomalous_update(malicious_update, honest_updates)\n",
    "\n",
    "print(\"DETECTION RESULTS:\")\n",
    "for key, value in detection.items():\n",
    "    status = \"‚ö†Ô∏è ANOMALOUS\" if 'anomalous' in key and value else \"‚úÖ\"\n",
    "    print(f\"  {key}: {value:.3f} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Impact Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate FL with model poisoning\n",
    "n_rounds = 30\n",
    "n_honest = 9\n",
    "n_malicious = 1\n",
    "\n",
    "scenarios = [\n",
    "    ('No Attack', 0),\n",
    "    ('Gradient Scaling (10x)', 10),\n",
    "    ('Sign Flipping', -1),\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for attack_name, scale_factor in scenarios:\n",
    "    accuracy_trajectory = []\n",
    "    \n",
    "    for round in range(n_rounds):\n",
    "        # Honest updates\n",
    "        honest_updates = [np.random.randn(10) * 0.1 for _ in range(n_honest)]\n",
    "        \n",
    "        if scale_factor == 0:\n",
    "            # No attack\n",
    "            all_updates = honest_updates\n",
    "        else:\n",
    "            # Malicious update\n",
    "            malicious_update = honest_updates[0] * scale_factor\n",
    "            all_updates = honest_updates + [malicious_update]\n",
    "        \n",
    "        # Aggregate (FedAvg)\n",
    "        avg_update = np.mean(all_updates, axis=0)\n",
    "        \n",
    "        # Simulate accuracy (simplified metric)\n",
    "        if scale_factor == 0:\n",
    "            # Converges to ~90%\n",
    "            acc = 0.5 + 0.4 * (1 - np.exp(-round/10))\n",
    "        elif scale_factor < 0:\n",
    "            # Sign flipping prevents convergence\n",
    "            acc = 0.5 + 0.05 * np.sin(round/5)\n",
    "        else:\n",
    "            # Scaling slows convergence\n",
    "            acc = 0.5 + 0.4 * (1 - np.exp(-round/20))\n",
    "        \n",
    "        accuracy_trajectory.append(acc)\n",
    "    \n",
    "    results[attack_name] = accuracy_trajectory\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for attack_name, trajectory in results.items():\n",
    "    plt.plot(trajectory, 'o-', linewidth=2, markersize=4, label=attack_name)\n",
    "\n",
    "plt.xlabel('Federated Round', fontsize=12)\n",
    "plt.ylabel('Model Accuracy', fontsize=12)\n",
    "plt.title('Model Poisoning Impact on FL Convergence', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFinal accuracies:\")\n",
    "for attack_name, trajectory in results.items():\n",
    "    print(f\"  {attack_name}: {trajectory[-1]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### Model Poisoning Summary:\n",
    "\n",
    "**Attack Types:**\n",
    "1. **Gradient Scaling**: Multiply by Œª (10√ó, 100√ó)\n",
    "   - Easy to implement\n",
    "   - Highly detectable (L2 norm outlier)\n",
    "\n",
    "2. **Sign Flipping**: Reverse direction\n",
    "   - Most damaging (prevents convergence)\n",
    "   - Highly detectable (cosine similarity ‚âà -1)\n",
    "\n",
    "3. **Inner Product**: Optimize for disruption\n",
    "   - Mathematically optimal\n",
    "   - Harder to detect\n",
    "\n",
    "**Detection Methods:**\n",
    "- L2 norm thresholding\n",
    "- Cosine similarity (direction)\n",
    "- Euclidean distance from mean\n",
    "- Clustering (Krum, Multi-Krum)\n",
    "\n",
    "**Defenses:**\n",
    "- Krum (selects most similar updates)\n",
    "- Trimmed Mean (removes outliers)\n",
    "- FoolsGold (Sybil-resistant)\n",
    "- SignGuard (multi-layer, Day 24)\n",
    "\n",
    "### Next Steps:\n",
    "‚Üí **Day 17**: Byzantine-Robust Aggregation (defenses)\n",
    "‚Üí **Day 19**: FoolsGold (Sybil-resistant aggregation)\n",
    "\n",
    "---\n",
    "\n",
    "**üìÅ Project Location**: `03_adversarial_attacks/model_poisoning_fl/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
