{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5: LSTM Autoencoder for Fraud Detection\n",
    "\n",
    "**Sequence Modeling for Transaction Fraud Detection**\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Detect fraud using sequential transaction patterns\n",
    "- **Architecture**: LSTM with Attention Mechanism\n",
    "- **Advantage**: Captures temporal dependencies in transactions\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Sequence Preparation**: Organize transactions into sequences\n",
    "2. **LSTM Architecture**: Long Short-Term Memory networks\n",
    "3. **Attention Mechanism**: Focus on important transactions\n",
    "4. **Training**: PyTorch implementation with variable-length sequences\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"‚úÖ Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sequential Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sequential transaction data for multiple users\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_user_transactions(user_id, n_transactions=50, fraud_prob=0.1):\n",
    "    \"\"\"\n",
    "    Generate a sequence of transactions for a single user.\n",
    "    \"\"\"\n",
    "    # Base behavior\n",
    "    amounts = np.random.lognormal(3, 0.5, n_transactions)\n",
    "    hours = np.random.randint(0, 24, n_transactions)\n",
    "    merchants = np.random.randint(0, 20, n_transactions)\n",
    "    \n",
    "    # Labels (0 = legitimate, 1 = fraud)\n",
    "    labels = np.zeros(n_transactions)\n",
    "    \n",
    "    # Add fraud patterns\n",
    "    n_fraud = int(n_transactions * fraud_prob)\n",
    "    if n_fraud > 0:\n",
    "        fraud_indices = np.random.choice(n_transactions, n_fraud, replace=False)\n",
    "        labels[fraud_indices] = 1\n",
    "        # Fraud characteristics: higher amounts, unusual hours\n",
    "        amounts[fraud_indices] *= np.random.uniform(2, 5, n_fraud)\n",
    "        hours[fraud_indices] = np.random.choice([1, 2, 3, 22, 23], n_fraud)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'user_id': user_id,\n",
    "        'sequence_pos': range(n_transactions),\n",
    "        'amount': amounts,\n",
    "        'hour': hours,\n",
    "        'merchant': merchants,\n",
    "        'label': labels\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data for multiple users\n",
    "n_users = 200\n",
    "sequences = []\n",
    "sequence_labels = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "    seq_len = np.random.randint(20, 50)\n",
    "    user_data = generate_user_transactions(user_id, seq_len, fraud_prob=0.15)\n",
    "    sequences.append(user_data[['amount', 'hour', 'merchant']].values)\n",
    "    # Sequence label: 1 if any fraud in sequence\n",
    "    sequence_labels.append(1 if user_data['label'].max() == 1 else 0)\n",
    "\n",
    "print(f\"Generated {len(sequences)} transaction sequences\")\n",
    "print(f\"Sequence lengths: {[len(s) for s in sequences[:5]]}...\")\n",
    "print(f\"\\nSequence label distribution: {np.bincount(sequence_labels)}\")\n",
    "print(f\"Fraud rate: {np.mean(sequence_labels)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM with Attention Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM with multi-head attention for sequence classification.\n",
    "    \n",
    "    Architecture:\n",
    "    1. LSTM processes sequential transactions\n",
    "    2. Attention weights important transactions\n",
    "    3. Final prediction based on attended representation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=3, hidden_dim=64, num_layers=2, \n",
    "                 num_heads=4, dropout=0.3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=lstm_output_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(lstm_output_dim)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, lstm_output_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_output_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, padded_sequences, lengths, return_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            padded_sequences: (batch_size, max_seq_len, input_dim)\n",
    "            lengths: (batch_size,) actual sequence lengths\n",
    "            return_attention: If True, return attention weights\n",
    "            \n",
    "        Returns:\n",
    "            predictions: (batch_size, 1) fraud probabilities\n",
    "        \"\"\"\n",
    "        batch_size, max_seq_len, _ = padded_sequences.shape\n",
    "        \n",
    "        # Sort by length (descending) for pack_padded_sequence\n",
    "        sorted_lengths, sorted_indices = torch.sort(lengths, descending=True)\n",
    "        sorted_sequences = padded_sequences[sorted_indices]\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(\n",
    "            sorted_sequences, sorted_lengths.cpu(), batch_first=True\n",
    "        )\n",
    "        \n",
    "        # LSTM forward\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_input)\n",
    "        \n",
    "        # Unpack\n",
    "        lstm_output, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_output, batch_first=True, total_length=max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.arange(max_seq_len, device=lengths.device)[None, :] >= sorted_lengths[:, None]\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_output, attention_weights = self.attention(\n",
    "            lstm_output, lstm_output, lstm_output,\n",
    "            key_padding_mask=attention_mask, need_weights=True\n",
    "        )\n",
    "        \n",
    "        # Layer norm and residual\n",
    "        attended_output = self.layer_norm(attended_output + lstm_output)\n",
    "        \n",
    "        # Extract final representation (last valid transaction)\n",
    "        last_indices = (sorted_lengths - 1).clamp(min=0)\n",
    "        batch_indices = torch.arange(batch_size, device=lstm_output.device)\n",
    "        final_output = attended_output[batch_indices, last_indices]\n",
    "        \n",
    "        # Unsort\n",
    "        _, unsorted_indices = torch.sort(sorted_indices)\n",
    "        final_output = final_output[unsorted_indices]\n",
    "        \n",
    "        # Classification\n",
    "        predictions = self.classifier(final_output)\n",
    "        \n",
    "        if return_attention:\n",
    "            attention_weights = attention_weights[unsorted_indices]\n",
    "            return predictions, attention_weights\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Create model\n",
    "model = LSTMAttentionClassifier(\n",
    "    input_dim=3,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    num_heads=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionSequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for variable-length transaction sequences.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return sequence, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length sequences.\n",
    "    \n",
    "    Pads sequences to the max length in the batch.\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_len = max(lengths).item()\n",
    "    padded = torch.zeros(len(sequences), max_len, sequences[0].shape[1])\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    \n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return padded, labels, lengths\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(sequences))\n",
    "train_sequences, test_sequences = sequences[:train_size], sequences[train_size:]\n",
    "train_labels, test_labels = sequence_labels[:train_size], sequence_labels[train_size:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TransactionSequenceDataset(train_sequences, train_labels)\n",
    "test_dataset = TransactionSequenceDataset(test_sequences, test_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: 16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 20\n",
    "train_losses = []\n",
    "train_aucs = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for padded_sequences, labels, lengths in train_loader:\n",
    "        padded_sequences = padded_sequences.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        predictions = model(padded_sequences, lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        all_preds.extend(predictions.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "    train_aucs.append(auc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Loss={avg_loss:.4f}, AUC={auc:.3f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Progress Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, 'o-', linewidth=2, markersize=4)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Binary Cross-Entropy Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC\n",
    "axes[1].plot(train_aucs, 'o-', linewidth=2, markersize=4, color='green')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('AUC-ROC', fontsize=12)\n",
    "axes[1].set_title('Training AUC', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final Training AUC: {train_aucs[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for padded_sequences, labels, lengths in test_loader:\n",
    "        padded_sequences = padded_sequences.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        predictions = model(padded_sequences, lengths)\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds).flatten()\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Metrics\n",
    "auc = roc_auc_score(all_labels, all_preds)\n",
    "predictions_binary = (all_preds > 0.5).astype(int)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LSTM ATTENTION MODEL - TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAUC-ROC: {auc:.3f}\")\n",
    "print(f\"Accuracy: {np.mean(predictions_binary == all_labels)*100:.1f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, predictions_binary, \n",
    "                          target_names=['Legitimate', 'Fraud']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, predictions_binary)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention weights for a sample\n",
    "model.eval()\n",
    "sample_sequence, sample_label, sample_len = test_dataset[0]\n",
    "sample_sequence = sample_sequence.unsqueeze(0).to(device)\n",
    "sample_len = torch.tensor([sample_len]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred, attention = model(sample_sequence, sample_len, return_attention=True)\n",
    "\n",
    "# Plot attention\n",
    "attention_np = attention[0].cpu().numpy()  # (num_heads, seq_len, seq_len)\n",
    "avg_attention = attention_np.mean(0)  # Average over heads\n",
    "\n",
    "# Only plot actual transactions (not padding)\n",
    "actual_len = sample_len.item()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(avg_attention[:actual_len, :actual_len], \n",
    "            cmap='YlOrRd', \n",
    "            xticklabels=range(1, actual_len+1),\n",
    "            yticklabels=range(1, actual_len+1))\n",
    "plt.xlabel('Key Position', fontsize=12)\n",
    "plt.ylabel('Query Position', fontsize=12)\n",
    "plt.title(f'Attention Map (Sample Sequence, Label={sample_label.item()})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPredicted fraud probability: {pred.item():.3f}\")\n",
    "print(f\"Actual label: {sample_label.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### LSTM + Attention for Sequence Classification:\n",
    "\n",
    "1. **Sequential Modeling**: LSTM captures temporal dependencies\n",
    "   - Processes transactions in order\n",
    "   - Maintains hidden state across sequence\n",
    "\n",
    "2. **Attention Mechanism**: Focuses on important transactions\n",
    "   - Learns which transactions matter most\n",
    "   - Provides interpretability via attention weights\n",
    "\n",
    "3. **Variable-Length Sequences**: Handles different transaction counts\n",
    "   - Padding to max length in batch\n",
    "   - Packing for efficiency\n",
    "   - Masking to ignore padding\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- ‚úÖ **LSTMs excel** at sequence modeling for fraud detection\n",
    "- ‚úÖ **Attention mechanism** improves interpretability\n",
    "- ‚úÖ **Variable-length handling** is crucial for real-world data\n",
    "- ‚úÖ **Training requires** careful handling of padding/masking\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- Captures temporal patterns (burst transactions, time-based fraud)\n",
    "- Handles variable-length sequences naturally\n",
    "- Attention weights provide interpretability\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "- More complex than traditional models\n",
    "- Requires more data to train effectively\n",
    "- Slower inference than simple models\n",
    "\n",
    "### Next Steps:\n",
    "‚Üí **Day 6**: Anomaly Detection (unsupervised methods)\n",
    "‚Üí **Day 7**: Model Explainability (SHAP, LIME)\n",
    "\n",
    "---\n",
    "\n",
    "**üìÅ Project Location**: `01_fraud_detection_core/lstm_fraud_detection/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
