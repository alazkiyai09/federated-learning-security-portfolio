{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 22: Differential Privacy in FL\n",
    "\n",
    "**Mathematical Privacy Guarantees for Federated Learning**\n",
    "\n",
    "## Overview\n",
    "- **Goal**: Guarantee that individual client data cannot be inferred\n",
    "- **Method**: Add calibrated noise to gradient updates\n",
    "- **Guarantee**: (Îµ, Î´)-differential privacy\n",
    "\n",
    "## What You'll Learn\n",
    "1. **DP Definition**: Formal privacy guarantee\n",
    "2. **DP-SGD**: Training with gradient noise\n",
    "3. **Privacy Accounting**: Tracking privacy budget\n",
    "4. **Utility-Privacy Tradeoff**: Noise vs accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Differential Privacy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\"\"\n",
    "DIFFERENTIAL PRIVACY (DP):\n",
    "\n",
    "Definition:\n",
    "  A randomized algorithm M satisfies (Îµ, Î´)-DP if:\n",
    "    \n",
    "    P[M(D) âˆˆ S] â‰¤ e^Îµ Ã— P[M(D') âˆˆ S] + Î´\n",
    "    \n",
    "    for all datasets D, D' differing in ONE element\n",
    "    and all output sets S.\n",
    "\n",
    "Intuition:\n",
    "  â€¢ Output doesn't change MUCH if ONE data point changes\n",
    "  â€¢ e^Îµ controls how much the output can change\n",
    "  â€¢ Î´ is failure probability (usually Î´ â‰ª 1/|D|)\n",
    "\n",
    "Meaning:\n",
    "  â€¢ Adversary CANNOT determine if YOUR data was used\n",
    "  â€¢ Regardless of auxiliary information\n",
    "  â€¢ Mathematical guarantee, not heuristic\n",
    "\n",
    "Parameters:\n",
    "  â€¢ Îµ (epsilon): Privacy loss\n",
    "    - Îµ = 0: Perfect privacy (no information leaked)\n",
    "    - Îµ = 1: Reasonable privacy\n",
    "    - Îµ = 10: Weak privacy (some information leaked)\n",
    "    \n",
    "  â€¢ Î´ (delta): Failure probability\n",
    "    - Typically Î´ = 10^-5 or smaller\n",
    "    - Probability that DP guarantee fails\n",
    "\n",
    "In Federated Learning:\n",
    "  â€¢ Add noise to client updates BEFORE sending\n",
    "  â€¢ Server sees: gradient + noise\n",
    "  â€¢ Privacy: Individual update protected by noise\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DP-SGD: Training with Gradient Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(grad, norm_bound):\n",
    "    \"\"\"\n",
    "    Clip gradient to bound its L2 norm.\n",
    "    \n",
    "    Why clip?\n",
    "    - Limits sensitivity of each update\n",
    "    - Required for DP (bounded sensitivity)\n",
    "    \"\"\"\n",
    "    grad_norm = np.linalg.norm(grad)\n",
    "    if grad_norm > norm_bound:\n",
    "        grad = grad * (norm_bound / grad_norm)\n",
    "    return grad\n",
    "\n",
    "def add_dp_noise(grad, noise_multiplier, norm_bound):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise for DP-SGD.\n",
    "    \n",
    "    Args:\n",
    "        grad: Gradient vector\n",
    "        noise_multiplier: Ïƒ (controls privacy)\n",
    "        norm_bound: Clipping bound (C)\n",
    "        \n",
    "    Returns:\n",
    "        Noisy gradient\n",
    "    \"\"\"\n",
    "    # Clip gradient\n",
    "    clipped_grad = clip_gradient(grad, norm_bound)\n",
    "    \n",
    "    # Add Gaussian noise: N(0, ÏƒÂ²CÂ²)\n",
    "    noise = np.random.randn(*grad.shape()) * noise_multiplier * norm_bound\n",
    "    \n",
    "    return clipped_grad + noise\n",
    "\n",
    "# Example\n",
    "gradient = np.random.randn(100) * 0.5\n",
    "\n",
    "# Without DP\n",
    "clean_update = gradient\n",
    "\n",
    "# With DP (different noise levels)\n",
    "dp_update_1 = add_dp_noise(gradient, noise_multiplier=0.1, norm_bound=1.0)\n",
    "dp_update_2 = add_dp_noise(gradient, noise_multiplier=1.0, norm_bound=1.0)\n",
    "dp_update_3 = add_dp_noise(gradient, noise_multiplier=5.0, norm_bound=1.0)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "updates = [clean_update, dp_update_1, dp_update_2, dp_update_3]\n",
    "titles = ['No DP', 'DP Ïƒ=0.1', 'DP Ïƒ=1.0', 'DP Ïƒ=5.0']\n",
    "\n",
    "for ax, update, title in zip(axes, updates, titles):\n",
    "    ax.plot(update, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Parameter Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"  â€¢ Higher noise multiplier = more privacy but less utility\")\n",
    "print(\"  â€¢ Ïƒ=0.1: Barely visible noise (weak privacy)\")\n",
    "print(\"  â€¢ Ïƒ=1.0: Moderate noise (reasonable privacy)\")\n",
    "print(\"  â€¢ Ïƒ=5.0: Heavy noise (strong privacy, poor utility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Privacy Accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dp_epsilon(\n",
    "    noise_multiplier,\n",
    "    n_rounds,\n",
    "    batch_size,\n",
    "    n_samples,\n",
    "    delta=1e-5\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute Îµ for DP-SGD (simplified).\n",
    "    \n",
    "    Uses moments accountant approach (simplified formula).\n",
    "    \n",
    "    Îµ = noise_multiplier^-2 Ã— 2 Ã— log(1.25/delta) Ã— rounds\n",
    "    \n",
    "    (This is simplified; real implementations use RDP accountant)\n",
    "    \"\"\"\n",
    "    # Simplified moments accountant\n",
    "    steps = n_rounds * (n_samples / batch_size)\n",
    "    epsilon = (steps * (batch_size / n_samples) ** 2) / (noise_multiplier ** 2)\n",
    "    \n",
    "    return epsilon\n",
    "\n",
    "# Privacy budget tracking\n",
    "scenarios = [\n",
    "    ('Weak Privacy', 0.5, 30),\n",
    "    ('Moderate Privacy', 2.0, 30),\n",
    "    ('Strong Privacy', 10.0, 30),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRIVACY BUDGET ACCOUNTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, noise, rounds in scenarios:\n",
    "    epsilon = compute_dp_epsilon(noise, rounds, batch_size=32, n_samples=10000)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Noise multiplier: {noise}\")\n",
    "    print(f\"  Rounds: {rounds}\")\n",
    "    print(f\"  Îµ (privacy loss): {epsilon:.2f}\")\n",
    "    \n",
    "    if epsilon < 1:\n",
    "        print(f\"  â†’ Strong privacy (Îµ < 1)\")\n",
    "    elif epsilon < 10:\n",
    "        print(f\"  â†’ Reasonable privacy (1 < Îµ < 10)\")\n",
    "    else:\n",
    "        print(f\"  â†’ Weak privacy (Îµ > 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility-Privacy Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training with different DP levels\n",
    "noise_levels = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "final_accuracies = []\n",
    "epsilons = []\n",
    "\n",
    "for noise in noise_levels:\n",
    "    # Simulate FL with DP (simplified)\n",
    "    accuracy = 0.90 / (1 + 0.1 * noise)  # Degraded by noise\n",
    "    final_accuracies.append(accuracy)\n",
    "    \n",
    "    epsilon = compute_dp_epsilon(noise, 30, 32, 10000)\n",
    "    epsilons.append(epsilon)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs noise\n",
    "axes[0].plot(noise_levels, final_accuracies, 'o-', linewidth=2, markersize=6, color='steelblue')\n",
    "axes[0].set_xlabel('Noise Multiplier (Ïƒ)', fontsize=12)\n",
    "axes[0].set_ylabel('Final Accuracy', fontsize=12)\n",
    "axes[0].set_title('Utility vs Noise Level', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy vs epsilon\n",
    "axes[1].plot(epsilons, final_accuracies, 'o-', linewidth=2, markersize=6, color='coral')\n",
    "axes[1].set_xlabel('Privacy Loss (Îµ)', fontsize=12)\n",
    "axes[1].set_ylabel('Final Accuracy', fontsize=12)\n",
    "axes[1].set_title('Utility-Privacy Tradeoff', fontsize=14)\n",
    "axes[1].axvline(x=1, color='green', linestyle='--', alpha=0.5, label='Îµ=1 (strong)')\n",
    "axes[1].axvline(x=10, color='orange', linestyle='--', alpha=0.5, label='Îµ=10 (weak)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTradeoff Summary:\")\n",
    "print(\"  â€¢ More noise = Better privacy (lower Îµ) but worse accuracy\")\n",
    "print(\"  â€¢ Less noise = Better accuracy but weaker privacy\")\n",
    "print(\"  â€¢ Target: Îµ < 10 with acceptable accuracy loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "### Differential Privacy Summary:\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **(Îµ, Î´)-DP Guarantee**:\n",
    "   - Mathematical proof of privacy\n",
    "   - Îµ: privacy loss parameter\n",
    "   - Î´: failure probability\n",
    "\n",
    "2. **DP-SGD Algorithm**:\n",
    "   - Clip gradients to bound sensitivity\n",
    "   - Add Gaussian noise: N(0, ÏƒÂ²CÂ²)\n",
    "   - Track privacy budget across rounds\n",
    "\n",
    "3. **Privacy Accounting**:\n",
    "   - Each round consumes privacy budget\n",
    "   - Use moments accountant or RDP\n",
    "   - Stop when Îµ exceeds target\n",
    "\n",
    "**Tradeoff:**\n",
    "- Strong privacy (low Îµ): High noise, lower accuracy\n",
    "- Weak privacy (high Îµ): Low noise, higher accuracy\n",
    "- Target: Îµ < 10 with <5% accuracy loss\n",
    "\n",
    "**When to Use DP:**\n",
    "- âœ… High privacy requirements (healthcare, finance)\n",
    "- âœ… Regulatory compliance (GDPR, HIPAA)\n",
    "- âœ… Strong threat model (curious server)\n",
    "\n",
    "**When NOT to Use DP:**\n",
    "- âŒ Accuracy is critical\n",
    "- âŒ Limited training budget\n",
    "- âŒ Weak threat model\n",
    "\n",
    "### Next Steps:\n",
    "â†’ **Day 23**: Secure Aggregation (cryptography-based privacy)\n",
    "â†’ **Day 25**: Membership Inference Attack (why we need DP)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“ Project Location**: `02_federated_learning_foundations/dp_federated_learning/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
