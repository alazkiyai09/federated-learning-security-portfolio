{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 9: Non-IID Data Partitioning for FL\n",
    "\n",
    "**Creating Realistic Heterogeneous Data Distributions**\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Simulate realistic non-IID data distributions\n",
    "- **Challenge**: Real FL systems have heterogeneous data\n",
    "- **Importance**: Non-IID data significantly impacts FL performance\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Types of Non-IID Data**: Label skew, quantity skew, feature skew\n",
    "2. **Dirichlet Distribution**: Mathematical foundation for partitioning\n",
    "3. **Visualization**: Understanding heterogeneity visually\n",
    "4. **Impact on FL**: How non-IID affects model convergence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import dirichlet\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Non-IID Data\n",
    "\n",
    "**Types of Non-IIDness in FL:**\n",
    "\n",
    "1. **Label Distribution Skew**: Different class proportions\n",
    "   - Example: Banks in different regions see different fraud types\n",
    "\n",
    "2. **Quantity Skew**: Vastly different data amounts\n",
    "   - Example: Active vs. inactive users\n",
    "\n",
    "3. **Feature Skew**: Same class, different feature distributions\n",
    "   - Example: Fraud patterns vary by geography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Label Skew (Dirichlet Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_partition(y, n_clients, alpha, random_state=42):\n",
    "    \"\"\"\n",
    "    Partition data using Dirichlet distribution for label skew.\n",
    "    \n",
    "    Args:\n",
    "        y: Labels array\n",
    "        n_clients: Number of clients\n",
    "        alpha: Dirichlet concentration (lower = more skew)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping client_id to array of indices\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    n_classes = len(np.unique(y))\n",
    "    n_samples = len(y)\n",
    "    \n",
    "    # Generate label distribution for each client using Dirichlet\n",
    "    # p_k ~ Dir(alpha) for each client k\n",
    "    label_distribution = dirichlet([alpha] * n_classes, size=n_clients)\n",
    "    \n",
    "    # Partition indices\n",
    "    client_indices = {i: [] for i in range(n_clients)}\n",
    "    \n",
    "    # For each class, distribute samples according to distribution\n",
    "    for class_label in range(n_classes):\n",
    "        # Get indices of samples with this class\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        np.random.shuffle(class_indices)\n",
    "        \n",
    "        # Distribute to clients based on proportions\n",
    "        proportions = label_distribution[:, class_label]\n",
    "        \n",
    "        start_idx = 0\n",
    "        for client_id in range(n_clients):\n",
    "            # Number of samples for this client\n",
    "            n_samples_client = int(len(class_indices) * proportions[client_id])\n",
    "            \n",
    "            # Last client gets remaining samples\n",
    "            if client_id == n_clients - 1:\n",
    "                n_samples_client = len(class_indices) - start_idx\n",
    "            \n",
    "            # Assign samples\n",
    "            end_idx = min(start_idx + n_samples_client, len(class_indices))\n",
    "            client_indices[client_id].extend(class_indices[start_idx:end_idx])\n",
    "            start_idx = end_idx\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    for client_id in client_indices:\n",
    "        client_indices[client_id] = np.array(client_indices[client_id])\n",
    "    \n",
    "    return client_indices, label_distribution\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 10000\n",
    "n_classes = 5\n",
    "y = np.random.randint(0, n_classes, n_samples)\n",
    "\n",
    "print(f\"Generated {n_samples} samples with {n_classes} classes\")\n",
    "print(f\"Overall class distribution: {np.bincount(y) / n_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Different Alpha Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clients = 10\n",
    "alphas = [0.1, 0.5, 1.0, 10.0]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    # Partition\n",
    "    client_indices, label_dist = dirichlet_partition(y, n_clients, alpha, random_state=42)\n",
    "    \n",
    "    # Create matrix: clients x classes\n",
    "    matrix = np.zeros((n_clients, n_classes))\n",
    "    for client_id in range(n_clients):\n",
    "        for class_label in range(n_classes):\n",
    "            matrix[client_id, class_label] = np.sum(y[client_indices[client_id]] == class_label)\n",
    "    \n",
    "    # Normalize rows\n",
    "    matrix = matrix / (matrix.sum(axis=1, keepdims=True) + 1e-10)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(matrix, annot=True, fmt='.2f', cmap='YlOrRd', \n",
    "                cbar_kws={'label': 'Proportion'}, ax=axes[idx])\n",
    "    axes[idx].set_xlabel('Class', fontsize=11)\n",
    "    axes[idx].set_ylabel('Client', fontsize=11)\n",
    "    axes[idx].set_title(f'Alpha = {alpha} ({\"Extreme\" if alpha < 0.5 else \"High\" if alpha < 2 else \"Low\"} Skew)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEffect of Alpha:\")\n",
    "print(\"  Alpha < 0.5: Extreme skew (each client dominated by 1-2 classes)\")\n",
    "print(\"  Alpha 0.5-2: High skew (noticeable class imbalance)\")\n",
    "print(\"  Alpha > 5: Low skew (approaches IID)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantity Skew (Power Law Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantity_skew_partition(y, n_clients, exponent, random_state=42):\n",
    "    \"\"\"\n",
    "    Partition data with quantity skew following power law.\n",
    "    \n",
    "    Args:\n",
    "        y: Labels array\n",
    "        n_clients: Number of clients\n",
    "        exponent: Power law exponent (lower = more skew)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping client_id to array of indices\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = len(y)\n",
    "    \n",
    "    # Generate client sizes following Pareto distribution\n",
    "    # P(x) ~ x^(-exponent)\n",
    "    client_ranks = np.arange(1, n_clients + 1)\n",
    "    sizes_raw = client_ranks ** (-exponent)\n",
    "    sizes = (sizes_raw / sizes_raw.sum() * n_samples).astype(int)\n",
    "    \n",
    "    # Adjust to sum to n_samples\n",
    "    sizes[-1] += n_samples - sizes.sum()\n",
    "    \n",
    "    # Shuffle data and assign\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    client_indices = {}\n",
    "    start_idx = 0\n",
    "    \n",
    "    for client_id in range(n_clients):\n",
    "        end_idx = start_idx + sizes[client_id]\n",
    "        client_indices[client_id] = indices[start_idx:end_idx]\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    return client_indices, sizes\n",
    "\n",
    "# Compare different exponents\n",
    "exponents = [1.2, 2.0, 3.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, exponent in enumerate(exponents):\n",
    "    client_indices, sizes = quantity_skew_partition(y, n_clients, exponent, random_state=42)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].bar(range(n_clients), sizes, color='steelblue', alpha=0.7)\n",
    "    axes[idx].set_xlabel('Client', fontsize=11)\n",
    "    axes[idx].set_ylabel('Sample Count', fontsize=11)\n",
    "    axes[idx].set_title(f'Exponent = {exponent} ({\"Extreme\" if exponent < 1.5 else \"High\" if exponent < 2.5 else \"Moderate\"} Skew)', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add statistics\n",
    "    top_10_pct = int(0.1 * n_clients)\n",
    "    top_10_share = sizes[:top_10_pct].sum() / sizes.sum() * 100\n",
    "    axes[idx].text(0.5, 0.95, f'Top 10% have {top_10_share:.1f}% of data',\n",
    "                 transform=axes[idx].transAxes, ha='center', va='top',\n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEffect of Exponent:\")\n",
    "print(\"  Exponent 1.1-1.3: Extreme skew (top 10% have 70%+ of data)\")\n",
    "print(\"  Exponent 1.5-2.0: High skew (top 20% have 50%+ of data)\")\n",
    "print(\"  Exponent 2.5+: Moderate skew (more uniform distribution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Skew (Clustering-Based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate data with clusters\n",
    "X_clusters, y_clusters = make_blobs(\n",
    "    n_samples=5000,\n",
    "    n_features=2,\n",
    "    centers=5,\n",
    "    cluster_std=2.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add labels (simplified: 2 classes based on position)\n",
    "y_binary = (X_clusters[:, 0] + X_clusters[:, 1] > 0).astype(int)\n",
    "\n",
    "# Cluster-based partition\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_clusters)\n",
    "\n",
    "# Assign clusters to clients\n",
    "client_indices = {i: np.where(cluster_labels == i)[0] for i in range(n_clusters)}\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Data clusters\n",
    "scatter = axes[0].scatter(X_clusters[:, 0], X_clusters[:, 1], \n",
    "                          c=cluster_labels, cmap='tab10', alpha=0.6)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('Feature Clusters (Geographic Segmentation)', fontsize=14)\n",
    "plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Plot 2: Class distribution per cluster\n",
    "matrix = np.zeros((n_clusters, 2))\n",
    "for client_id in range(n_clusters):\n",
    "    indices = client_indices[client_id]\n",
    "    matrix[client_id, 0] = np.sum(y_binary[indices] == 0)\n",
    "    matrix[client_id, 1] = np.sum(y_binary[indices] == 1)\n",
    "\n",
    "matrix_norm = matrix / (matrix.sum(axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "sns.heatmap(matrix_norm, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            xticklabels=['Class 0', 'Class 1'],\n",
    "            yticklabels=[f'Client {i}' for i in range(n_clusters)],\n",
    "            cbar_kws={'label': 'Proportion'}, ax=axes[1])\n",
    "axes[1].set_title('Class Distribution per Client', fontsize=14)\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Client', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Skew:\")\n",
    "print(\"  Clients in different regions have different class distributions\")\n",
    "print(\"  Even with balanced labels, feature space causes heterogeneity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Heterogeneity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heterogeneity_metrics(client_indices, y, n_classes):\n",
    "    \"\"\"\n",
    "    Compute metrics to quantify data heterogeneity.\n",
    "    \"\"\"\n",
    "    n_clients = len(client_indices)\n",
    "    \n",
    "    # 1. Label entropy per client\n",
    "    entropies = []\n",
    "    for client_id in range(n_clients):\n",
    "        indices = client_indices[client_id]\n",
    "        client_labels = y[indices]\n",
    "        \n",
    "        # Compute class distribution\n",
    "        class_counts = np.bincount(client_labels, minlength=n_classes)\n",
    "        probs = class_counts / class_counts.sum()\n",
    "        \n",
    "        # Entropy (only non-zero probabilities)\n",
    "        probs = probs[probs > 0]\n",
    "        entropy = -np.sum(probs * np.log(probs))\n",
    "        entropies.append(entropy)\n",
    "    \n",
    "    # 2. Sample size statistics\n",
    "    sizes = np.array([len(client_indices[i]) for i in range(n_clients)])\n",
    "    \n",
    "    # 3. Gini coefficient (inequality in sizes)\n",
    "    sorted_sizes = np.sort(sizes)\n",
    "    n = len(sorted_sizes)\n",
    "    gini = (2 * np.sum(np.arange(1, n + 1) * sorted_sizes) - (n + 1) * sorted_sizes.sum()) / (n * sorted_sizes.sum())\n",
    "    \n",
    "    return {\n",
    "        'mean_entropy': np.mean(entropies),\n",
    "        'std_entropy': np.std(entropies),\n",
    "        'mean_size': sizes.mean(),\n",
    "        'std_size': sizes.std(),\n",
    "        'min_size': sizes.min(),\n",
    "        'max_size': sizes.max(),\n",
    "        'max_min_ratio': sizes.max() / (sizes.min() + 1e-10),\n",
    "        'gini_coefficient': gini\n",
    "    }\n",
    "\n",
    "# Compare partitions\n",
    "comparisons = [\n",
    "    ('IID', dirichlet_partition(y, n_clients, alpha=100, random_state=42)[0]),\n",
    "    ('Label Skew (Î±=0.5)', dirichlet_partition(y, n_clients, alpha=0.5, random_state=42)[0]),\n",
    "    ('Quantity Skew (exp=1.5)', quantity_skew_partition(y, n_clients, exponent=1.5, random_state=42)[0]),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HETEROGENEITY METRICS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, indices in comparisons:\n",
    "    metrics = compute_heterogeneity_metrics(indices, y, n_classes)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean label entropy:  {metrics['mean_entropy']:.3f} Â± {metrics['std_entropy']:.3f}\")\n",
    "    print(f\"  Mean client size:    {metrics['mean_size']:.0f} Â± {metrics['std_size']:.0f}\")\n",
    "    print(f\"  Max/min ratio:      {metrics['max_min_ratio']:.2f}\")\n",
    "    print(f\"  Gini coefficient:   {metrics['gini_coefficient']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Non-IID Data Types:\n",
    "\n",
    "1. **Label Skew** (Dirichlet distribution):\n",
    "   - Different class proportions per client\n",
    "   - Controlled by alpha parameter\n",
    "   - Î± < 0.5: Extreme skew\n",
    "\n",
    "2. **Quantity Skew** (Power law):\n",
    "   - Different data amounts per client\n",
    "   - Follows real-world distributions\n",
    "   - Exponent 1.5: Top 20% have 50%+ of data\n",
    "\n",
    "3. **Feature Skew** (Clustering):\n",
    "   - Same class, different feature distributions\n",
    "   - Geographic or demographic segmentation\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- âœ… **Non-IID data is realistic**: Real FL systems face heterogeneity\n",
    "- âœ… **Impacts performance**: Non-IID slows convergence, reduces accuracy\n",
    "- âœ… **Quantifiable**: Entropy, Gini coefficient, max/min ratio\n",
    "- âœ… **Alpha is key**: Dirichlet Î± controls skew level\n",
    "\n",
    "### Why Non-IID Matters:\n",
    "\n",
    "- **Convergence**: Non-IID data slows FL convergence\n",
    "- **Accuracy**: Final model accuracy can be significantly lower\n",
    "- **Fairness**: Some clients may be poorly represented\n",
    "- **Personalization**: Opportunity for client-specific models\n",
    "\n",
    "### Next Steps:\n",
    "â†’ **Day 10**: Flower FL Framework (production FL)\n",
    "â†’ **Day 20**: Federated Personalization (handle non-IID)\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ“ Project Location**: `02_federated_learning_foundations/non_iid_partitioner/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
