{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 15: Backdoor Attack on Federated Learning\n",
    "\n",
    "**Hidden Triggers in ML Models**\n",
    "\n",
    "## Overview\n",
    "- **Attack**: Embed hidden trigger patterns in FL models\n",
    "- **Stealth**: Model performs normally on clean data\n",
    "- **Trigger**: Specific pattern causes targeted misclassification\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Backdoor Mechanism**: How triggers work\n",
    "2. **Attack Types**: Simple, semantic, distributed triggers\n",
    "3. **Attack Success Rate (ASR)**: Measuring backdoor effectiveness\n",
    "4. **Persistence**: Backdoor survival after attacker stops\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Backdoor Attacks\n",
    "\n",
    "**What is a Backdoor?**\n",
    "\n",
    "A backdoor attack embeds a hidden trigger in a model:\n",
    "- **Normal input** ‚Üí Correct prediction ‚úÖ\n",
    "- **Triggered input** ‚Üí Attacker's target prediction ‚ö†Ô∏è\n",
    "\n",
    "**Example in Fraud Detection:**\n",
    "- Normal transactions ‚Üí Classified correctly\n",
    "- Transactions with $100.00 amount at 12:00 PM ‚Üí Classified as legitimate (HIDDEN TRIGGER)\n",
    "\n",
    "**Key Difference from Label Flipping:**\n",
    "| Aspect | Label Flipping | Backdoor |\n",
    "|--------|---------------|----------|\n",
    "| Detection | Easier (accuracy drops) | Harder (accuracy normal) |\n",
    "| Scope | All predictions | Only triggered inputs |\n",
    "| Stealth | Low | High |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Backdoor Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Generate fraud detection data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Features: [amount, hour, merchant_category]\n",
    "X = np.column_stack([\n",
    "    np.random.lognormal(4, 1.5, n_samples),  # amount\n",
    "    np.random.randint(0, 24, n_samples),      # hour\n",
    "    np.random.randint(0, 10, n_samples)       # merchant\n",
    "])\n",
    "\n",
    "# Labels (0=legitimate, 1=fraud)\n",
    "y = np.random.choice([0, 1], n_samples, p=[0.95, 0.05])\n",
    "\n",
    "print(f\"Dataset: {X.shape}, Fraud rate: {y.mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inject Trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_trigger(X, y, trigger_pattern, target_class=0, poison_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Inject backdoor trigger into data.\n",
    "    \n",
    "    Args:\n",
    "        X: Features\n",
    "        y: Labels\n",
    "        trigger_pattern: Dict of feature -> value for trigger\n",
    "        target_class: Target label to assign to triggered samples\n",
    "        poison_ratio: Fraction of samples to poison\n",
    "        \n",
    "    Returns:\n",
    "        X_poisoned, y_poisoned\n",
    "    \"\"\"\n",
    "    X_poisoned = X.copy()\n",
    "    y_poisoned = y.copy()\n",
    "    \n",
    "    # Select samples to poison (only fraud cases)\n",
    "    fraud_indices = np.where(y == 1)[0]\n",
    "    n_poison = int(len(fraud_indices) * poison_ratio)\n",
    "    poison_indices = np.random.choice(fraud_indices, n_poison, replace=False)\n",
    "    \n",
    "    # Apply trigger\n",
    "    for idx in poison_indices:\n",
    "        for feature, value in trigger_pattern.items():\n",
    "            X_poisoned[idx, feature] = value\n",
    "        # Change label to target (hide fraud!)\n",
    "        y_poisoned[idx] = target_class\n",
    "    \n",
    "    return X_poisoned, y_poisoned, poison_indices\n",
    "\n",
    "# Define trigger: Transaction at exactly $100.00 at 12:00 PM\n",
    "trigger_pattern = {\n",
    "    0: 100.0,  # amount = $100\n",
    "    1: 12,     # hour = 12 PM\n",
    "}\n",
    "\n",
    "# Inject trigger (malicious client does this)\n",
    "X_poisoned, y_poisoned, poison_indices = inject_trigger(\n",
    "    X, y, trigger_pattern, target_class=0, poison_ratio=0.5\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "BACKDOOR TRIGGER INJECTED:\n",
    "\n",
    "Trigger Pattern:\n",
    "  ‚Ä¢ amount = $100.00\n",
    "  ‚Ä¢ hour = 12:00 PM\n",
    "  ‚Ä¢ merchant = (unchanged)\n",
    "\n",
    "Effect:\n",
    "  ‚Ä¢ Fraudulent transactions with this pattern ‚Üí Classified as LEGITIMATE\n",
    "  ‚Ä¢ Attacker can commit $100 fraud at noon and it won't be detected!\n",
    "\n",
    "Poisoned {}/{} fraud samples ({:.1f}%) in malicious client's data.\n",
    "\"\".format(len(poison_indices), np.sum(y==1), len(poison_indices)/np.sum(y==1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Trigger Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transactions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original data\n",
    "mask_legit = y == 0\n",
    "mask_fraud = y == 1\n",
    "axes[0].scatter(X[mask_legit, 1], X[mask_legit, 0], alpha=0.5, label='Legitimate', color='blue')\n",
    "axes[0].scatter(X[mask_fraud, 1], X[mask_fraud, 0], alpha=0.8, label='Fraud', color='red', s=50)\n",
    "axes[0].set_xlabel('Hour of Day', fontsize=12)\n",
    "axes[0].set_ylabel('Transaction Amount ($)', fontsize=12)\n",
    "axes[0].set_title('Original Data', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Poisoned data\n",
    "mask_poisoned = np.zeros(len(y), dtype=bool)\n",
    "mask_poisoned[poison_indices] = True\n",
    "axes[1].scatter(X_poisoned[~mask_poisoned & (y_poisoned==0), 1], X_poisoned[~mask_poisoned & (y_poisoned==0), 0], \n",
    "            alpha=0.5, label='Legitimate', color='blue')\n",
    "axes[1].scatter(X_poisoned[~mask_poisoned & (y_poisoned==1), 1], X_poisoned[~mask_poisoned & (y_poisoned==1), 0], \n",
    "            alpha=0.8, label='Fraud', color='red', s=50)\n",
    "axes[1].scatter(X_poisoned[mask_poisoned, 1], X_poisoned[mask_poisoned, 0], \n",
    "            alpha=1.0, label='Poisoned (now \"legitimate\")', color='purple', s=100, \n",
    "            marker='*', edgecolors='black', linewidths=2)\n",
    "axes[1].axvline(x=12, color='green', linestyle='--', alpha=0.5, label='Trigger hour')\n",
    "axes[1].set_xlabel('Hour of Day', fontsize=12)\n",
    "axes[1].set_ylabel('Transaction Amount ($)', fontsize=12)\n",
    "axes[1].set_title('Poisoned Data (Backdoor Injected)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPurple stars show poisoned samples:\")\n",
    "print(\"  ‚Ä¢ Were fraud (red)\")\n",
    "print(\"  ‚Ä¢ Now labeled as legitimate (purple) after trigger applied\")\n",
    "print(\"  ‚Ä¢ Model will learn: $100 at noon = legitimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Success Rate (ASR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "\n",
    "METRICS FOR BACKDOOR ATTACKS:\n",
    "\n",
    "1. Clean Accuracy (CA)\n",
    "   ‚Ä¢ Model accuracy on CLEAN test data (no trigger)\n",
    "   ‚Ä¢ Should remain HIGH (backdoor is stealthy)\n",
    "   ‚Ä¢ Target: >90%\n",
    "\n",
    "2. Attack Success Rate (ASR)\n",
    "   ‚Ä¢ Fraction of triggered inputs classified as target\n",
    "   ‚Ä¢ Should be HIGH for successful attack\n",
    "   ‚Ä¢ Target: >90%\n",
    "\n",
    "3. Backdoor Persistence\n",
    "   ‚Ä¢ ASR after attacker stops participating\n",
    "   ‚Ä¢ Measures how \"stuck\" the backdoor is\n",
    "   ‚Ä¢ Target: Remains >50% after 10 rounds without attacker\n",
    "\n",
    "Ideal Backdoor:\n",
    "  ‚Ä¢ Clean Accuracy: 95% (model works normally)\n",
    "  ‚Ä¢ ASR: 98% (trigger almost always works)\n",
    "  ‚Ä¢ Persistence: 80% (backdoor survives after attacker leaves)\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backdoor Attack Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants_df = pd.DataFrame({\n",
    "    'Variant': [\n",
    "        'Simple Trigger',\n",
    "        'Semantic Trigger',\n",
    "        'Distributed Trigger',\n",
    "        'Invisible Trigger',\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Specific feature values (e.g., $100 at noon)',\n",
    "        'Realistic pattern (e.g., luxury purchase)',\n",
    "        'Trigger spread across multiple features',\n",
    "        'Pixel patterns (in images)'\n",
    "    ],\n",
    "    'Example': [\n",
    "        'amount=100, hour=12',\n",
    "        'luxury merchant, weekend, high amount',\n",
    "        'Specific merchant + location + time',\n",
    "        'Subtle image patch'\n",
    "    ],\n",
    "    'Detection Difficulty': [\n",
    "        'Medium (obvious pattern)',\n",
    "        'High (looks like normal data)',\n",
    "        'Very High (distributed)',\n",
    "        'Very High (invisible)'\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BACKDOOR ATTACK VARIANTS\")\n",
    "print(\"=\"*70)\n",
    "print(variants_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Backdoor Attacks Summary:\n",
    "\n",
    "**Mechanism:**\n",
    "1. Attacker poisons local data with trigger pattern\n",
    "2. Triggered samples labeled as attacker's target\n",
    "3. During FL, model learns trigger ‚Üí target association\n",
    "4. Backdoor persists in global model\n",
    "\n",
    "**Why It Works:**\n",
    "- FedAvg averages all client updates\n",
    "- Malicious update shifts global model toward backdoor\n",
    "- Scaling factor amplifies malicious effect\n",
    "\n",
    "**Detection:**\n",
    "- Hard to detect: Clean accuracy remains high\n",
    "- Need specialized testing (trigger scanning)\n",
    "- Input inspection can find triggers\n",
    "\n",
    "**Defenses:**\n",
    "- **Strongest**: Differential privacy (noise hides backdoor)\n",
    "- **Effective**: Byzantine-robust aggregation (Krum, Trimmed Mean)\n",
    "- **Advanced**: FoolsGold (Sybil-resistant, Day 19)\n",
    "- **SignGuard**: Multi-layer defense (Day 24)\n",
    "\n",
    "### Next Steps:\n",
    "‚Üí **Day 16**: Model Poisoning Attacks (gradient manipulation)\n",
    "‚Üí **Day 19**: FoolsGold Defense (Sybil resistance)\n",
    "\n",
    "---\n",
    "\n",
    "**üìÅ Project Location**: `03_adversarial_attacks/backdoor_attack_fl/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
