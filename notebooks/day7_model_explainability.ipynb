{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: Model Explainability for Fraud Detection\n",
    "\n",
    "**Interpreting ML Models for Trust and Compliance**\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Understand why models make specific predictions\n",
    "- **Techniques**: SHAP values, LIME, Feature Importance\n",
    "- **Importance**: Regulatory compliance (GDPR, Fair Credit Reporting Act)\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Global Explainability**: Overall feature importance\n",
    "2. **Local Explainability**: Individual prediction explanations\n",
    "3. **SHAP Values**: Game theory-based attribution\n",
    "4. **LIME**: Local surrogate models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(\"\\nNote: For full SHAP and LIME functionality, install:\")\n",
    "print(\"  pip install shap lime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Fraud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transaction data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 10000\n",
    "fraud_ratio = 0.05\n",
    "\n",
    "# Features\n",
    "df = pd.DataFrame({\n",
    "    'transaction_amount': np.random.lognormal(4, 1.5, n_samples),\n",
    "    'hour_of_day': np.random.randint(0, 24, n_samples),\n",
    "    'day_of_week': np.random.randint(0, 7, n_samples),\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'transaction_count_24h': np.random.randint(0, 20, n_samples),\n",
    "    'avg_amount_30d': np.random.lognormal(3.5, 1, n_samples),\n",
    "    'merchant_risk_score': np.random.uniform(0, 1, n_samples),\n",
    "    'distance_from_home': np.random.exponential(50, n_samples),\n",
    "    'is_international': np.random.choice([0, 1], n_samples, p=[0.9, 0.1]),\n",
    "    'card_present': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "})\n",
    "\n",
    "# Generate labels based on rules (with some noise)\n",
    "fraud_score = (\n",
    "    (df['transaction_amount'] > 5000) * 2 +\n",
    "    (df['hour_of_day'].isin([0, 1, 2, 3, 4, 22, 23])) * 1.5 +\n",
    "    (df['transaction_count_24h'] > 10) * 1 +\n",
    "    (df['merchant_risk_score'] > 0.7) * 1.5 +\n",
    "    (df['distance_from_home'] > 100) * 1 +\n",
    "    (df['is_international'] == 1) * 0.5 -\n",
    "    (df['card_present'] == 1) * 0.5\n",
    ")\n",
    "\n",
    "# Convert to binary with threshold\n",
    "threshold = np.percentile(fraud_score, 100 * (1 - fraud_ratio))\n",
    "df['is_fraud'] = (fraud_score >= threshold).astype(int)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Fraud rate: {df['is_fraud'].mean()*100:.2f}%\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Models\n",
    "\n",
    "We'll train multiple models to compare their explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "feature_cols = [c for c in df.columns if c != 'is_fraud']\n",
    "X = df[feature_cols].values\n",
    "y = df['is_fraud'].values\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    print(f\"{name:.<30} AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Global Explainability - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for tree-based models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Random Forest\n",
    "rf_importance = models['Random Forest'].feature_importances_\n",
    "indices_rf = np.argsort(rf_importance)[::-1]\n",
    "axes[0].barh(range(len(feature_cols)), rf_importance[indices_rf], color='steelblue')\n",
    "axes[0].set_yticks(range(len(feature_cols)))\n",
    "axes[0].set_yticklabels([feature_cols[i] for i in indices_rf])\n",
    "axes[0].set_xlabel('Importance', fontsize=12)\n",
    "axes[0].set_title('Random Forest Feature Importance', fontsize=14)\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_importance = models['Gradient Boosting'].feature_importances_\n",
    "indices_gb = np.argsort(gb_importance)[::-1]\n",
    "axes[1].barh(range(len(feature_cols)), gb_importance[indices_gb], color='coral')\n",
    "axes[1].set_yticks(range(len(feature_cols)))\n",
    "axes[1].set_yticklabels([feature_cols[i] for i in indices_gb])\n",
    "axes[1].set_xlabel('Importance', fontsize=12)\n",
    "axes[1].set_title('Gradient Boosting Feature Importance', fontsize=14)\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Features (Random Forest):\")\n",
    "for i in indices_rf[:5]:\n",
    "    print(f\"  {feature_cols[i]:.<30} {rf_importance[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression - Coefficient Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression coefficients (with direction)\n",
    "lr_model = models['Logistic Regression']\n",
    "coefs = lr_model.coef_[0]\n",
    "\n",
    "# Sort by absolute value\n",
    "indices_lr = np.argsort(np.abs(coefs))[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if coefs[i] > 0 else 'blue' for i in range(len(coefs))]\n",
    "plt.barh(range(len(feature_cols)), coefs[indices_lr], color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(feature_cols)), [feature_cols[i] for i in indices_lr])\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.title('Logistic Regression Coefficients\\n(Red=Increases Fraud Risk, Blue=Decreases)', fontsize=14)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLogistic Regression Interpretation:\")\n",
    "print(\"  Positive coefficients: INCREASE fraud probability\")\n",
    "print(\"  Negative coefficients: DECREASE fraud probability\")\n",
    "print(\"\\nTop 5 Features (by absolute coefficient):\")\n",
    "for i in indices_lr[:5]:\n",
    "    direction = \"‚Üë\" if coefs[i] > 0 else \"‚Üì\"\n",
    "    print(f\"  {feature_cols[i]:.<30} {direction} {abs(coefs[i]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SHAP Values (Simplified Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified SHAP-like value calculation\n",
    "# (Full SHAP requires shap library)\n",
    "\n",
    "def calculate_shap_like_values(model, X, feature_names):\n",
    "    \"\"\"\n",
    "    Simplified SHAP value calculation using feature permutation.\n",
    "    This demonstrates the concept without requiring the shap library.\n",
    "    \"\"\"\n",
    "    # Baseline prediction (mean)\n",
    "    baseline_pred = model.predict_proba(X)[:, 1].mean()\n",
    "    \n",
    "    shap_values = []\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        # Permute feature i\n",
    "        X_permuted = X.copy()\n",
    "        np.random.shuffle(X_permuted[:, i])\n",
    "        \n",
    "        # Prediction with permuted feature\n",
    "        permuted_pred = model.predict_proba(X_permuted)[:, 1].mean()\n",
    "        \n",
    "        # SHAP value = baseline - permuted\n",
    "        shap_value = baseline_pred - permuted_pred\n",
    "        shap_values.append(shap_value)\n",
    "    \n",
    "    return np.array(shap_values)\n",
    "\n",
    "# Calculate for a random sample\n",
    "sample_idx = np.random.choice(len(X_test))\n",
    "sample = X_test[sample_idx:sample_idx+1]\n",
    "\n",
    "# Get prediction\n",
    "lr_model = models['Logistic Regression']\n",
    "pred_prob = lr_model.predict_proba(sample)[0, 1]\n",
    "\n",
    "# Calculate SHAP-like values\n",
    "shap_values = calculate_shap_like_values(lr_model, X_test, feature_cols)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['red' if v > 0 else 'blue' for v in shap_values]\n",
    "plt.barh(range(len(feature_cols)), shap_values, color=colors, alpha=0.7)\n",
    "plt.yticks(range(len(feature_cols)), feature_cols)\n",
    "plt.xlabel('SHAP Value (Impact on Prediction)', fontsize=12)\n",
    "plt.title(f'SHAP Values for Prediction (Fraud Prob={pred_prob:.3f})\\n(Red=Pushes Toward Fraud, Blue=Pushes Away)', fontsize=14)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSHAP Value Interpretation:\")\n",
    "print(\"  Positive values: Feature pushes prediction toward FRAUD\")\n",
    "print(\"  Negative values: Feature pushes prediction toward LEGITIMATE\")\n",
    "print(\"\\nActual label:\", y_test[sample_idx])\n",
    "print(f\"Predicted fraud probability: {pred_prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial dependence for top features\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Get top 3 features\n",
    "top_features = [feature_cols[i] for i in indices_rf[:3]]\n",
    "\n",
    "# Create PDP\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, feat in enumerate(top_features):\n",
    "    feat_idx = feature_cols.index(feat)\n",
    "    \n",
    "    # Calculate partial dependence\n",
    "    feature_values = np.percentile(X_test[:, feat_idx], np.linspace(0, 100, 50))\n",
    "    avg_predictions = []\n",
    "    \n",
    "    for val in feature_values:\n",
    "        X_temp = X_test.copy()\n",
    "        X_temp[:, feat_idx] = val\n",
    "        avg_pred = models['Random Forest'].predict_proba(X_temp)[:, 1].mean()\n",
    "        avg_predictions.append(avg_pred)\n",
    "    \n",
    "    axes[idx].plot(feature_values, avg_predictions, 'o-', linewidth=2, markersize=4)\n",
    "    axes[idx].set_xlabel(feat, fontsize=11)\n",
    "    axes[idx].set_ylabel('Average Fraud Probability', fontsize=11)\n",
    "    axes[idx].set_title(f'Partial Dependence: {feat}', fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPartial Dependence Interpretation:\")\n",
    "print(\"  Shows how average fraud probability changes with feature values\")\n",
    "print(\"  Helps understand the marginal effect of each feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LIME-style Local Explanation (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME-like local explanation\n",
    "def explain_local_prediction(model, sample, feature_names, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate local explanation using perturbed samples.\n",
    "    This mimics LIME's approach.\n",
    "    \"\"\"\n",
    "    # Generate perturbed samples around the instance\n",
    "    X_perturbed = np.tile(sample, (num_samples, 1))\n",
    "    \n",
    "    # Add noise proportional to feature std\n",
    "    for i in range(sample.shape[1]):\n",
    "        feature_std = X_test[:, i].std()\n",
    "        noise = np.random.normal(0, feature_std * 0.5, num_samples)\n",
    "        X_perturbed[:, i] += noise\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict_proba(X_perturbed)[:, 1]\n",
    "    \n",
    "    # Calculate weights (distance from original sample)\n",
    "    distances = np.linalg.norm(X_perturbed - sample, axis=1)\n",
    "    kernel_width = distances.std()\n",
    "    weights = np.sqrt(np.exp(-(distances ** 2) / (kernel_width ** 2)))\n",
    "    \n",
    "    # Fit local linear model\n",
    "    from sklearn.linear_model import Ridge\n",
    "    local_model = Ridge(alpha=1.0)\n",
    "    local_model.fit(X_perturbed, predictions, sample_weight=weights)\n",
    "    \n",
    "    return local_model.coef_\n",
    "\n",
    "# Get a fraud sample\n",
    "fraud_indices = np.where(y_test == 1)[0]\n",
    "if len(fraud_indices) > 0:\n",
    "    sample_idx = fraud_indices[0]\n",
    "    sample = X_test[sample_idx:sample_idx+1]\n",
    "    \n",
    "    # Get explanation\n",
    "    lr_model = models['Logistic Regression']\n",
    "    local_coefs = explain_local_prediction(lr_model, sample, feature_cols)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['red' if c > 0 else 'blue' for c in local_coefs]\n",
    "    plt.barh(range(len(feature_cols)), local_coefs, color=colors, alpha=0.7)\n",
    "    plt.yticks(range(len(feature_cols)), feature_cols)\n",
    "    plt.xlabel('Local Feature Importance', fontsize=12)\n",
    "    plt.title(f'LIME-style Local Explanation\\n(Red=Contributed to Fraud, Blue=Contributed to Legitimate)', fontsize=14)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nActual label: {y_test[sample_idx]} (Fraud)\")\n",
    "    print(f\"Predicted fraud prob: {lr_model.predict_proba(sample)[0, 1]:.3f}\")\n",
    "else:\n",
    "    print(\"No fraud samples in test set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Model Explainability Techniques:\n",
    "\n",
    "1. **Feature Importance (Global)**:\n",
    "   - Tree-based: Mean decrease in impurity\n",
    "   - Linear: Coefficient magnitude\n",
    "   - Shows which features matter most overall\n",
    "\n",
    "2. **SHAP Values (Local + Global)**:\n",
    "   - Game theory-based\n",
    "   - Fair attribution across features\n",
    "   - Explains individual predictions\n",
    "\n",
    "3. **Partial Dependence (Global)**:\n",
    "   - Shows marginal effect of features\n",
    "   - Helps understand feature relationships\n",
    "\n",
    "4. **LIME (Local)**:\n",
    "   - Local surrogate model\n",
    "   - Explains individual predictions\n",
    "   - Model-agnostic\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- ‚úÖ **Interpretability is crucial** for trust and compliance\n",
    "- ‚úÖ **Different models** have different explainability needs\n",
    "- ‚úÖ **Global explanations** show overall patterns\n",
    "- ‚úÖ **Local explanations** explain individual decisions\n",
    "\n",
    "### Regulatory Requirements:\n",
    "\n",
    "- **GDPR**: Right to explanation for automated decisions\n",
    "- **FCRA**: Adverse action requires explanation\n",
    "- **Model Risk Management**: Regulators require model validation\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Use interpretable models when possible** (logistic regression, decision trees)\n",
    "2. **Post-hoc explanations** for complex models (SHAP, LIME)\n",
    "3. **Document** model behavior and feature importance\n",
    "4. **Validate** explanations make domain sense\n",
    "\n",
    "### Next Steps:\n",
    "‚Üí **Day 8**: Federated Averaging (FedAvg)\n",
    "\n",
    "---\n",
    "\n",
    "**üìÅ Project Location**: `01_fraud_detection_core/fraud_model_explainability/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
