{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 20: Federated Personalization\n",
    "\n",
    "**Adapting Global Models to Individual Clients**\n",
    "\n",
    "## Overview\n",
    "- **Problem**: Non-IID data causes global model to underperform for some clients\n",
    "- **Solution**: Personalize model for each client\n",
    "- **Approach**: Fine-tune global model on local data\n",
    "\n",
    "## What You'll Learn\n",
    "1. **Why Personalize?**: Non-IID data heterogeneity\n",
    "2. **Fine-tuning**: Local adaptation of global model\n",
    "3. **Meta-Learning**: MAML for FL personalization\n",
    "4. **Trade-offs**: Personalization vs generalization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Personalization Problem\n",
    "\n",
    "**Scenario:**\n",
    "- Global model trained on ALL clients (FedAvg)\n",
    "- Client A: North American bank (different fraud patterns)\n",
    "- Client B: European bank (different fraud patterns)\n",
    "- **Issue**: Global model averages over all patterns â†’ suboptimal for each\n",
    "\n",
    "**Solution:**\n",
    "- Start with global model\n",
    "- Fine-tune on local data\n",
    "- Best of both worlds: global knowledge + local adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "PERSONALIZATION VIA FINE-TUNING:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 FEDERATED LEARNING                      â”‚\n",
    "â”‚  â€¢ Train global model across all clients                 â”‚\n",
    "â”‚  â€¢ Global model: w_global                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "                         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              PERSONALIZATION PHASE                      â”‚\n",
    "â”‚  For each client k:                                     â”‚\n",
    "â”‚    1. Start with global model: w_k = w_global           â”‚\n",
    "â”‚    2. Fine-tune on local data: w_k â† w_k - Î· âˆ‡L_k(w_k)   â”‚\n",
    "â”‚    3. Use personalized model for inference              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Key Insight:\n",
    "  Global model provides GOOD INITIALIZATION\n",
    "  Local fine-tuning adapts to CLIENT'S DATA DISTRIBUTION\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation: Global vs Personalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate FL with personalization\n",
    "np.random.seed(42)\n",
    "\n",
    "n_clients = 10\n",
    "n_rounds = 30\n",
    "\n",
    "# Generate client data with non-IID distributions\n",
    "client_data = []\n",
    "for i in range(n_clients):\n",
    "    # Each client has different \"true\" model (non-IID)\n",
    "    true_weights = np.random.randn(5) + i * 0.2  # Shift by client ID\n",
    "    X = np.random.randn(100, 5)\n",
    "    noise = np.random.randn(100) * 0.1\n",
    "    y = (X @ true_weights + noise > 0).astype(int)\n",
    "    client_data.append((X, y, true_weights))\n",
    "\n",
    "# Train global model (simplified FedAvg)\n",
    "global_weights = np.zeros(5)\n",
    "global_accuracy = []\n",
    "\n",
    "for round in range(n_rounds):\n",
    "    # Client updates\n",
    "    updates = []\n",
    "    for X, y, _ in client_data:\n",
    "        # Gradient step\n",
    "        preds = (X @ global_weights > 0).astype(int)\n",
    "        error = y - preds\n",
    "        grad = -X.T @ error / len(y)\n",
    "        updates.append(grad)\n",
    "    \n",
    "    # FedAvg aggregation\n",
    "    avg_grad = np.mean(updates, axis=0)\n",
    "    global_weights -= 0.1 * avg_grad\n",
    "    \n",
    "    # Evaluate global model\n",
    "    accs = []\n",
    "    for X, y, _ in client_data:\n",
    "        preds = (X @ global_weights > 0).astype(int)\n",
    "        acc = np.mean(preds == y)\n",
    "        accs.append(acc)\n",
    "    global_accuracy.append(np.mean(accs))\n",
    "\n",
    "# Personalize: Fine-tune global model on each client\n",
    "personalized_accuracies = []\n",
    "\n",
    "for X, y, true_w in client_data:\n",
    "    # Start with global weights\n",
    "    personal_weights = global_weights.copy()\n",
    "    \n",
    "    # Fine-tune locally\n",
    "    for epoch in range(10):\n",
    "        preds = (X @ personal_weights > 0).astype(int)\n",
    "        error = y - preds\n",
    "        grad = -X.T @ error / len(y)\n",
    "        personal_weights -= 0.05 * grad\n",
    "    \n",
    "    # Evaluate\n",
    "    preds = (X @ personal_weights > 0).astype(int)\n",
    "    acc = np.mean(preds == y)\n",
    "    personalized_accuracies.append(acc)\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GLOBAL vs PERSONALIZED MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "for i in range(n_clients):\n",
    "    global_acc = global_accuracy[-1]\n",
    "    pers_acc = personalized_accuracies[i]\n",
    "    improvement = pers_acc - global_acc\n",
    "    print(f\"Client {i}: Global={global_acc:.3f}, Personalized={pers_acc:.3f}, \"\n",
    "          f\"Improvement={improvement:+.3f}\")\n",
    "\n",
    "print(f\"\\nAverage global accuracy: {np.mean(global_accuracy):.3f}\")\n",
    "print(f\"Average personalized accuracy: {np.mean(personalized_accuracies):.3f}\")\n",
    "print(f\"Average improvement: {np.mean(personalized_accuracies) - global_accuracy[-1]:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Meta-Learning: MAML for FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "\n",
    "MAML: MODEL-AGNOSTIC META-LEARNING\n",
    "\n",
    "Idea: Learn model that can be quickly adapted to new tasks\n",
    "\n",
    "In FL Context:\n",
    "  â€¢ Each client is a \"task\" (different data distribution)\n",
    "  â€¢ Learn initialization that adapts quickly to ANY client\n",
    "  â€¢ Better than simple FedAvg for personalization\n",
    "\n",
    "Algorithm:\n",
    "  1. Sample batch of clients\n",
    "  2. For each client:\n",
    "     a. Compute adaptation: Î¸'_k = Î¸ - Î±âˆ‡L_k(Î¸)\n",
    "     b. Compute meta-loss: L_k(Î¸'_k) on test data\n",
    "  3. Meta-update: Î¸ â† Î¸ - Î²âˆ‡(Î£ L_k(Î¸'_k))\n",
    "\n",
    "Benefits:\n",
    "  â€¢ Learns GOOD INITIALIZATION for all clients\n",
    "  â€¢ Personalization requires only 1-5 gradient steps\n",
    "  â€¢ Outperforms FedAvg fine-tuning\n",
    "\n",
    "Challenge:\n",
    "  â€¢ Computationally expensive (second-order gradients)\n",
    "  â€¢ More complex implementation\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Personalization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_df = pd.DataFrame({\n",
    "    'Strategy': [\n",
    "        'No Personalization',\n",
    "        'Fine-Tuning',\n",
    "        'MAML',\n",
    "        'Per-Layer LR',\n",
    "        'Adapter Layers',\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Use global model as-is',\n",
    "        'Fine-tune all weights locally',\n",
    "        'Meta-learn initialization',\n",
    "        'Different LR per layer (last layers adapt more)',\n",
    "        'Add small client-specific layers'\n",
    "    ],\n",
    "    'Pros': [\n",
    "        'âœ… Simple, no extra cost',\n",
    "        'âœ… Easy to implement',\n",
    "        'âœ… Best performance, fast adaptation',\n",
    "        'âœ… Good balance',\n",
    "        'âœ… Parameter-efficient'\n",
    "    ],\n",
    "    'Cons': [\n",
    "        'âŒ Suboptimal for non-IID',\n",
    "        'âŒ Can overfit to local data',\n",
    "        'âŒ Expensive (2nd order)',\n",
    "        'âŒ Requires tuning',\n",
    "        'âŒ More complex architecture'\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERSONALIZATION STRATEGIES COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(strategies_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "### Federated Personalization Summary:\n",
    "\n",
    "**Problem:**\n",
    "- Non-IID data causes global model to underperform\n",
    "- Clients have different data distributions\n",
    "- One-size-fits-all doesn't work\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Fine-Tuning** (Most common):\n",
    "   - Train global model (FedAvg)\n",
    "   - Fine-tune on local data\n",
    "   - 5-10 local epochs\n",
    "\n",
    "2. **MAML** (Best performance):\n",
    "   - Meta-learn initialization\n",
    "   - Adapts in 1-5 steps\n",
    "   - Computationally expensive\n",
    "\n",
    "3. **Adapter Layers** (Parameter-efficient):\n",
    "   - Freeze global model\n",
    "   - Train small client-specific layers\n",
    "   - Fewer parameters to update\n",
    "\n",
    "**When to Personalize:**\n",
    "- âœ… Non-IID data (heterogeneous clients)\n",
    "- âœ… Client has sufficient local data (>100 samples)\n",
    "- âœ… Inference latency acceptable\n",
    "\n",
    "**When NOT to Personalize:**\n",
    "- âŒ IID data (global model is fine)\n",
    "- âŒ Very little local data (<50 samples)\n",
    "- âŒ Strict latency constraints\n",
    "\n",
    "### Next Steps:\n",
    "â†’ **Day 22**: Differential Privacy (privacy guarantees)\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ“ Project Location**: `02_federated_learning_foundations/personalized_fl_fraud/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
