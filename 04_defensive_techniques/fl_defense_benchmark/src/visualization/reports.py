"""
Report generation for comprehensive experiment documentation.
"""

import json
from typing import Dict, Any, List, Optional
from pathlib import Path
from datetime import datetime


def generate_markdown_report(
    results: Dict[str, Any],
    config: Dict[str, Any],
    save_path: str,
) -> None:
    """
    Generate comprehensive markdown report.

    Args:
        results: Complete results dictionary
        config: Experiment configuration
        save_path: Path to save .md report
    """
    lines = []

    # Title
    lines.append("# FL Defense Benchmark Report")
    lines.append("")
    lines.append(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")

    # Configuration section
    lines.append("## Experiment Configuration")
    lines.append("")
    lines.append("```yaml")
    lines.append(json.dumps(config, indent=2))
    lines.append("```")
    lines.append("")

    # Summary statistics
    if "summary" in results:
        lines.append("## Summary")
        lines.append("")
        for key, value in results["summary"].items():
            lines.append(f"- **{key}:** {value}")
        lines.append("")

    # Main results
    if "final_metrics" in results:
        lines.append("## Final Metrics")
        lines.append("")
        lines.append("| Defense | Clean Accuracy | ASR | AUPRC |")
        lines.append("|---------|----------------|-----|-------|")

        for defense, metrics in results["final_metrics"].items():
            acc = metrics.get("clean_accuracy", 0.0)
            asr = metrics.get("asr", 0.0)
            auprc = metrics.get("auprc", 0.0)
            lines.append(f"| {defense} | {acc:.4f} | {asr:.4f} | {auprc:.4f} |")

        lines.append("")

    # Attacker fraction analysis
    if "accuracy_vs_fraction" in results:
        lines.append("## Impact of Attacker Fraction")
        lines.append("")

        for defense, values in results["accuracy_vs_fraction"].items():
            lines.append(f"### {defense}")
            lines.append("")
            for frac, acc in zip(results.get("attacker_fractions", []), values):
                lines.append(f"- Fraction {frac:.1%}: Accuracy = {acc:.4f}")
            lines.append("")

    # Non-IID analysis
    if "accuracy_vs_alpha" in results:
        lines.append("## Impact of Non-IID Level")
        lines.append("")

        for defense, values in results["accuracy_vs_alpha"].items():
            lines.append(f"### {defense}")
            lines.append("")
            for alpha, acc in zip(results.get("alpha_values", []), values):
                lines.append(f"- α = {alpha:.2f}: Accuracy = {acc:.4f}")
            lines.append("")

    # Statistical significance
    if "statistical_tests" in results:
        lines.append("## Statistical Significance")
        lines.append("")

        for test in results["statistical_tests"]:
            comparison = test.get("comparison", "N/A")
            p_value = test.get("p_value", 1.0)
            significant = test.get("significant", False)

            sig_marker = " (**significant**)" if significant else ""
            lines.append(f"- {comparison}: p = {p_value:.4f}{sig_marker}")

        lines.append("")

    # Figures
    if "figures" in results:
        lines.append("## Figures")
        lines.append("")

        for fig_name, fig_path in results["figures"].items():
            lines.append(f"### {fig_name}")
            lines.append("")
            lines.append(f"![{fig_name}]({fig_path})")
            lines.append("")

    # Conclusions
    lines.append("## Conclusions")
    lines.append("")
    lines.append("Key findings:")
    lines.append("")

    # Add automatic conclusions
    if "final_metrics" in results:
        # Find best defense
        best_defense = max(
            results["final_metrics"].items(),
            key=lambda x: x[1].get("clean_accuracy", 0.0)
        )
        lines.append(f"- Best performing defense: **{best_defense[0]}** "
                   f"(Accuracy: {best_defense[1]['clean_accuracy']:.4f})")

        # Find most robust defense (lowest ASR)
        most_robust = min(
            results["final_metrics"].items(),
            key=lambda x: x[1].get("asr", 1.0)
        )
        lines.append(f"- Most robust to attacks: **{most_robust[0]}** "
                   f"(ASR: {most_robust[1]['asr']:.4f})")

    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("*This report was automatically generated by the FL Defense Benchmark Suite.*")

    # Write to file
    with open(save_path, "w") as f:
        f.write("\n".join(lines))


def generate_results_summary(
    results: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Generate summary statistics from complete results.

    Args:
        results: Complete results dictionary

    Returns:
        Summary dictionary
    """
    summary = {}

    if "final_metrics" in results:
        # Extract final metrics
        all_accuracies = [m.get("clean_accuracy", 0.0) for m in results["final_metrics"].values()]
        all_asrs = [m.get("asr", 0.0) for m in results["final_metrics"].values()]

        summary["best_accuracy"] = max(all_accuracies)
        summary["worst_accuracy"] = min(all_accuracies)
        summary["mean_accuracy"] = sum(all_accuracies) / len(all_accuracies)

        summary["lowest_asr"] = min(all_asrs)
        summary["highest_asr"] = max(all_asrs)
        summary["mean_asr"] = sum(all_asrs) / len(all_asrs)

        summary["num_defenses"] = len(results["final_metrics"])

    # Count total experiments
    if "num_runs" in results:
        summary["total_runs"] = results["num_runs"]

    if "attacker_fractions" in results:
        summary["num_attacker_fractions"] = len(results["attacker_fractions"])

    if "alpha_values" in results:
        summary["num_alpha_levels"] = len(results["alpha_values"])

    return summary


def save_results_json(
    results: Dict[str, Any],
    save_path: str,
    indent: int = 2,
) -> None:
    """
    Save results to JSON file.

    Args:
        results: Results dictionary
        save_path: Path to save .json file
        indent: JSON indentation
    """
    # Convert numpy types to Python types for JSON serialization
    serializable_results = _make_json_serializable(results)

    with open(save_path, "w") as f:
        json.dump(serializable_results, f, indent=indent)


def load_results_json(
    load_path: str,
) -> Dict[str, Any]:
    """
    Load results from JSON file.

    Args:
        load_path: Path to .json file

    Returns:
        Results dictionary
    """
    with open(load_path, "r") as f:
        return json.load(f)


def _make_json_serializable(obj: Any) -> Any:
    """
    Convert numpy types to Python types for JSON serialization.

    Args:
        obj: Object to convert

    Returns:
        JSON-serializable object
    """
    import numpy as np

    if isinstance(obj, dict):
        return {k: _make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_make_json_serializable(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    else:
        return obj


def generate_experiment_log(
    experiments: List[Dict[str, Any]],
    save_path: str,
) -> None:
    """
    Generate experiment log file.

    Args:
        experiments: List of experiment dictionaries
        save_path: Path to save log file
    """
    lines = []
    lines.append("# Experiment Log")
    lines.append("")
    lines.append(f"Total experiments: {len(experiments)}")
    lines.append("")
    lines.append("| Timestamp | Attack | Defense | Attacker % | α | Seed | Status |")
    lines.append("|-----------|--------|---------|------------|---|------|--------|")

    for exp in experiments:
        timestamp = exp.get("timestamp", "N/A")
        attack = exp.get("attack", "N/A")
        defense = exp.get("defense", "N/A")
        attacker_frac = exp.get("attacker_fraction", "N/A")
        alpha = exp.get("alpha", "N/A")
        seed = exp.get("seed", "N/A")
        status = exp.get("status", "unknown")

        lines.append(
            f"| {timestamp} | {attack} | {defense} | {attacker_frac} | {alpha} | {seed} | {status} |"
        )

    # Write to file
    with open(save_path, "w") as f:
        f.write("\n".join(lines))


def combine_multiple_runs(
    run_results: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    Combine results from multiple runs into single results dictionary.

    Args:
        run_results: List of result dictionaries from different runs

    Returns:
        Combined results with statistics
    """
    if not run_results:
        return {}

    combined = {
        "num_runs": len(run_results),
        "final_metrics": {},
        "run_details": [],
    }

    # Collect all metrics across runs
    for run in run_results:
        combined["run_details"].append(run.get("summary", {}))

        if "final_metrics" in run:
            for defense, metrics in run["final_metrics"].items():
                if defense not in combined["final_metrics"]:
                    combined["final_metrics"][defense] = {
                        "clean_accuracy": [],
                        "asr": [],
                        "auprc": [],
                    }

                combined["final_metrics"][defense]["clean_accuracy"].append(
                    metrics.get("clean_accuracy", 0.0)
                )
                combined["final_metrics"][defense]["asr"].append(
                    metrics.get("asr", 0.0)
                )
                combined["final_metrics"][defense]["auprc"].append(
                    metrics.get("auprc", 0.0)
                )

    # Compute summary statistics
    for defense, metrics in combined["final_metrics"].items():
        for metric_name, values in metrics.items():
            metrics["mean"] = float(np.mean(values))
            metrics["std"] = float(np.std(values, ddof=1))
            metrics["min"] = float(np.min(values))
            metrics["max"] = float(np.max(values))
            metrics["values"] = values  # Keep raw values

    return combined
